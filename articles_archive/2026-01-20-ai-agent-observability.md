---
title: "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Observabilityå®Œå…¨ã‚¬ã‚¤ãƒ‰ - æœ¬ç•ªé‹ç”¨ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¨è©•ä¾¡"
emoji: "ğŸ”­"
type: "tech"
topics: ["ai", "observability", "llm", "ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°", "ç›£è¦–"]
published: false
---

**ã€Œæœ¬ç•ªã§å‹•ã„ã¦ã„ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã€æŠŠæ¡ã§ãã¦ã„ã¾ã™ã‹ï¼Ÿã€**

LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ¬ç•ªé‹ç”¨ã§æœ€ã‚‚è¦‹è½ã¨ã•ã‚ŒãŒã¡ãªã®ãŒ**Observabilityï¼ˆå¯è¦³æ¸¬æ€§ï¼‰**ã§ã™ã€‚

2026å¹´ã®ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æœ¬ç•ªé‹ç”¨ã—ã¦ã„ã‚‹çµ„ç¹”ã®**94%**ãŒä½•ã‚‰ã‹ã®å¯è¦³æ¸¬æ€§ãƒ„ãƒ¼ãƒ«ã‚’å°å…¥ã—ã€**71.5%**ãŒè©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Observabilityã‚’ä½“ç³»çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## ãªãœã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ObservabilityãŒå¿…è¦ã‹

### å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¨ã®é•ã„

```
å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢:
Input â†’ [æ±ºå®šè«–çš„å‡¦ç†] â†’ Output
- åŒã˜å…¥åŠ› â†’ åŒã˜å‡ºåŠ›
- ãƒ‡ãƒãƒƒã‚°: ãƒ­ã‚°ã¨ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ:
Input â†’ [LLMåˆ¤æ–­] â†’ [ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ] â†’ [LLMåˆ¤æ–­] â†’ ... â†’ Output
- åŒã˜å…¥åŠ› â†’ ç•°ãªã‚‹å‡ºåŠ›ï¼ˆéæ±ºå®šè«–çš„ï¼‰
- ä¸­é–“ã®ã€Œæ€è€ƒã€ãŒè¦‹ãˆãªã„
- å¤±æ•—åŸå› ã®ç‰¹å®šãŒå›°é›£
```

### å¯è¦³æ¸¬æ€§ãŒãªã„ã¨ã©ã†ãªã‚‹ã‹

```
å•é¡Œç™ºç”Ÿæ™‚:

ã‚µãƒãƒ¼ãƒˆ: ã€Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé–“é•ã£ãŸå›ç­”ã‚’ã—ã¾ã—ãŸã€
é–‹ç™ºè€…: ã€Œãƒ­ã‚°ã‚’è¦‹ã¾ã™...ã€

ãƒ­ã‚°:
[INFO] Request received
[INFO] Response sent

é–‹ç™ºè€…: ã€Œä½•ãŒèµ·ããŸã‹å…¨ãåˆ†ã‹ã‚Šã¾ã›ã‚“ã€
```

---

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆObservabilityã®3ã¤ã®æŸ±

### 1. Tracingï¼ˆãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼‰

**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œãƒ‘ã‚¹ã‚’è¿½è·¡**

```
Trace: user_request_123
â”œâ”€â”€ Span: parse_input (2ms)
â”œâ”€â”€ Span: llm_call_1 (1,234ms)
â”‚   â”œâ”€â”€ prompt: "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æã—ã¦..."
â”‚   â”œâ”€â”€ completion: "ã“ã®è³ªå•ã¯è£½å“æ¤œç´¢ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã™..."
â”‚   â””â”€â”€ tokens: {input: 150, output: 50}
â”œâ”€â”€ Span: tool_call:search_products (456ms)
â”‚   â”œâ”€â”€ args: {query: "é’ã„Tã‚·ãƒ£ãƒ„", limit: 10}
â”‚   â””â”€â”€ result: [{id: 1, name: "..."}, ...]
â”œâ”€â”€ Span: llm_call_2 (987ms)
â”‚   â”œâ”€â”€ prompt: "æ¤œç´¢çµæœã‚’å…ƒã«å›ç­”ã‚’ç”Ÿæˆ..."
â”‚   â””â”€â”€ completion: "ä»¥ä¸‹ã®è£½å“ãŒãŠã™ã™ã‚ã§ã™..."
â””â”€â”€ Span: format_response (5ms)
```

### 2. Metricsï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰

**æ•°å€¤ã§æ¸¬å®šå¯èƒ½ãªæŒ‡æ¨™**

```python
metrics = {
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    "latency_p50": 1.2,  # ç§’
    "latency_p99": 5.8,
    "throughput": 100,    # req/min

    # ã‚³ã‚¹ãƒˆ
    "tokens_per_request": 2500,
    "cost_per_request": 0.05,  # ãƒ‰ãƒ«

    # å“è³ª
    "success_rate": 0.95,
    "hallucination_rate": 0.02,
    "user_satisfaction": 4.2,  # 5ç‚¹æº€ç‚¹

    # ã‚¨ãƒ©ãƒ¼
    "error_rate": 0.03,
    "timeout_rate": 0.01,
}
```

### 3. Evaluationï¼ˆè©•ä¾¡ï¼‰

**å‡ºåŠ›ã®å“è³ªã‚’å®šé‡çš„ã«è©•ä¾¡**

```python
evaluation_dimensions = {
    "relevance": "å›ç­”ã¯è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ã‹",
    "faithfulness": "å›ç­”ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã‹ï¼ˆå¹»è¦šãªã—ï¼‰",
    "coherence": "å›ç­”ã¯è«–ç†çš„ã§ä¸€è²«ã—ã¦ã„ã‚‹ã‹",
    "helpfulness": "å›ç­”ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç”¨ã‹",
}
```

---

## ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®å®Ÿè£…

### OpenTelemetryãƒ™ãƒ¼ã‚¹ã®è¨­è¨ˆ

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer("ai-agent")

# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼è¨­å®šï¼ˆLangfuse, Datadog, Grafanaç­‰ã«é€ä¿¡å¯èƒ½ï¼‰
exporter = OTLPSpanExporter(endpoint="http://collector:4317")
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(exporter)
)
```

### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿

```python
import functools
from opentelemetry import trace

tracer = trace.get_tracer("ai-agent")

def trace_llm_call(func):
    """LLMå‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        with tracer.start_as_current_span("llm_call") as span:
            # å…¥åŠ›ã‚’è¨˜éŒ²
            span.set_attribute("llm.model", kwargs.get("model", "unknown"))
            span.set_attribute("llm.prompt", str(kwargs.get("prompt", ""))[:1000])

            try:
                result = await func(*args, **kwargs)

                # å‡ºåŠ›ã‚’è¨˜éŒ²
                span.set_attribute("llm.completion", str(result)[:1000])
                span.set_attribute("llm.tokens.input", result.usage.input_tokens)
                span.set_attribute("llm.tokens.output", result.usage.output_tokens)
                span.set_status(trace.Status(trace.StatusCode.OK))

                return result

            except Exception as e:
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise

    return wrapper

def trace_tool_call(func):
    """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        with tracer.start_as_current_span(f"tool:{func.__name__}") as span:
            span.set_attribute("tool.name", func.__name__)
            span.set_attribute("tool.args", str(kwargs)[:500])

            try:
                result = await func(*args, **kwargs)
                span.set_attribute("tool.result", str(result)[:500])
                span.set_status(trace.Status(trace.StatusCode.OK))
                return result

            except Exception as e:
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                raise

    return wrapper
```

### ä½¿ç”¨ä¾‹

```python
class Agent:
    @trace_llm_call
    async def think(self, prompt):
        return await self.llm.generate(prompt=prompt)

    @trace_tool_call
    async def search_database(self, query):
        return await self.db.search(query)

    async def run(self, user_input):
        with tracer.start_as_current_span("agent_run") as span:
            span.set_attribute("user.input", user_input)

            # æ€è€ƒ
            thought = await self.think(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")

            # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
            results = await self.search_database(thought.query)

            # å›ç­”ç”Ÿæˆ
            response = await self.think(f"çµæœ: {results}")

            span.set_attribute("agent.response", str(response)[:500])
            return response
```

---

## è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…

### LLM-as-Judge

äººé–“ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒé›£ã—ã„å ´åˆã€LLMã§è©•ä¾¡ï¼š

```python
class LLMJudge:
    def __init__(self, evaluator_llm):
        self.llm = evaluator_llm

    async def evaluate_relevance(self, question, answer, context=None):
        """å›ç­”ã®é–¢é€£æ€§ã‚’è©•ä¾¡ (1-5)"""
        prompt = f"""
        ä»¥ä¸‹ã®è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

        è³ªå•: {question}
        å›ç­”: {answer}
        {"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: " + context if context else ""}

        å›ç­”ã¯è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ã§ã™ã‹ï¼Ÿ
        1-5ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã€ç†ç”±ã‚‚è¿°ã¹ã¦ãã ã•ã„ã€‚

        å‡ºåŠ›å½¢å¼:
        ã‚¹ã‚³ã‚¢: [1-5]
        ç†ç”±: [èª¬æ˜]
        """

        result = await self.llm.generate(prompt)
        return self._parse_score(result)

    async def evaluate_faithfulness(self, answer, context):
        """å¹»è¦šãŒãªã„ã‹è©•ä¾¡"""
        prompt = f"""
        ä»¥ä¸‹ã®å›ç­”ãŒã€ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}
        å›ç­”: {answer}

        å›ç­”ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã‹ï¼Ÿ
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€Œå¹»è¦šã‚ã‚Šã€ã¨åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        å‡ºåŠ›å½¢å¼:
        åˆ¤å®š: [å¹»è¦šãªã— / å¹»è¦šã‚ã‚Š]
        è©²å½“ç®‡æ‰€: [å¹»è¦šãŒã‚ã‚‹å ´åˆã€ãã®éƒ¨åˆ†ã‚’å¼•ç”¨]
        """

        result = await self.llm.generate(prompt)
        return self._parse_faithfulness(result)

    async def evaluate_all(self, question, answer, context=None):
        """è¤‡æ•°ã®è¦³ç‚¹ã§ä¸€æ‹¬è©•ä¾¡"""
        results = await asyncio.gather(
            self.evaluate_relevance(question, answer, context),
            self.evaluate_faithfulness(answer, context) if context else None,
            self.evaluate_coherence(answer),
            self.evaluate_helpfulness(question, answer),
        )

        return {
            "relevance": results[0],
            "faithfulness": results[1],
            "coherence": results[2],
            "helpfulness": results[3],
        }
```

### äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã¨ã®çµ„ã¿åˆã‚ã›

```python
class HybridEvaluator:
    def __init__(self, llm_judge, human_review_queue):
        self.llm = llm_judge
        self.human_queue = human_review_queue

    async def evaluate(self, interaction):
        # 1. ã¾ãšLLMã§è‡ªå‹•è©•ä¾¡
        llm_scores = await self.llm.evaluate_all(
            interaction.question,
            interaction.answer,
            interaction.context,
        )

        # 2. ä½ã‚¹ã‚³ã‚¢ã¾ãŸã¯é‡è¦ãªã‚±ãƒ¼ã‚¹ã¯äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¸
        needs_human_review = (
            llm_scores["relevance"] < 3 or
            llm_scores["faithfulness"] == "å¹»è¦šã‚ã‚Š" or
            interaction.is_high_stakes
        )

        if needs_human_review:
            await self.human_queue.add(interaction, llm_scores)

        return {
            "llm_scores": llm_scores,
            "needs_human_review": needs_human_review,
        }
```

---

## ç•°å¸¸æ¤œå‡ºã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–

```python
class AgentMonitor:
    def __init__(self, alert_thresholds):
        self.thresholds = alert_thresholds
        self.metrics = MetricsCollector()

    async def check_health(self):
        """å®šæœŸçš„ã«ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        current_metrics = await self.metrics.get_current()

        alerts = []

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç•°å¸¸
        if current_metrics["latency_p99"] > self.thresholds["latency_p99"]:
            alerts.append(Alert(
                severity="warning",
                message=f"P99 latency {current_metrics['latency_p99']}s exceeds threshold",
            ))

        # ã‚¨ãƒ©ãƒ¼ç‡ç•°å¸¸
        if current_metrics["error_rate"] > self.thresholds["error_rate"]:
            alerts.append(Alert(
                severity="critical",
                message=f"Error rate {current_metrics['error_rate']:.2%} exceeds threshold",
            ))

        # å¹»è¦šç‡ç•°å¸¸
        if current_metrics["hallucination_rate"] > self.thresholds["hallucination_rate"]:
            alerts.append(Alert(
                severity="critical",
                message=f"Hallucination rate {current_metrics['hallucination_rate']:.2%} is high",
            ))

        # ã‚³ã‚¹ãƒˆç•°å¸¸
        if current_metrics["cost_per_hour"] > self.thresholds["cost_per_hour"]:
            alerts.append(Alert(
                severity="warning",
                message=f"Hourly cost ${current_metrics['cost_per_hour']:.2f} exceeds budget",
            ))

        return alerts
```

### ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

```python
class DriftDetector:
    """å‡ºåŠ›å“è³ªã®ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º"""

    def __init__(self, baseline_metrics, window_size=1000):
        self.baseline = baseline_metrics
        self.window = deque(maxlen=window_size)

    def record(self, evaluation_result):
        self.window.append(evaluation_result)

    def detect_drift(self):
        if len(self.window) < 100:
            return None  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³

        current_metrics = self._calculate_current_metrics()

        drifts = []
        for metric_name, baseline_value in self.baseline.items():
            current_value = current_metrics.get(metric_name)

            if current_value is None:
                continue

            # çµ±è¨ˆçš„æœ‰æ„å·®ã‚’æ¤œå®š
            drift_score = self._calculate_drift_score(
                baseline_value,
                current_value,
            )

            if drift_score > 0.05:  # 5%ä»¥ä¸Šã®å¤‰åŒ–
                drifts.append({
                    "metric": metric_name,
                    "baseline": baseline_value,
                    "current": current_value,
                    "drift_score": drift_score,
                })

        return drifts if drifts else None
```

---

## 2026å¹´ã®Observabilityãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

### ä¸»è¦ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¯”è¼ƒ

| ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  | ç‰¹å¾´ | ä¾¡æ ¼å¸¯ |
|----------------|------|--------|
| **Langfuse** | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå¯èƒ½ | ç„¡æ–™ã€œ |
| **LangSmith** | LangChainçµ±åˆã€ä½¿ã„ã‚„ã™ã„ | $39/æœˆã€œ |
| **Braintrust** | è‡ªå‹•è©•ä¾¡ã€ã‚³ã‚¹ãƒˆåˆ†æ | Enterprise |
| **Arize Phoenix** | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º | ç„¡æ–™ã€œ |
| **Datadog LLM** | æ—¢å­˜ã®Datadogçµ±åˆ | å¾“é‡èª²é‡‘ |

### Langfuseã®å®Ÿè£…ä¾‹

```python
from langfuse import Langfuse
from langfuse.decorators import observe

# åˆæœŸåŒ–
langfuse = Langfuse()

class ObservedAgent:
    @observe(name="agent_run")
    async def run(self, user_input):
        # ãƒˆãƒ¬ãƒ¼ã‚¹ã¯è‡ªå‹•ã§è¨˜éŒ²ã•ã‚Œã‚‹

        thought = await self.think(user_input)
        results = await self.search(thought.query)
        response = await self.respond(results)

        # ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²
        langfuse.score(
            trace_id=langfuse.get_current_trace_id(),
            name="user_feedback",
            value=await self.get_user_feedback(),
        )

        return response

    @observe(name="llm_call", capture_input=True, capture_output=True)
    async def think(self, prompt):
        return await self.llm.generate(prompt)

    @observe(name="tool_call:search")
    async def search(self, query):
        return await self.db.search(query)
```

---

## CI/CDã¸ã®çµ±åˆ

### è©•ä¾¡ãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–

```python
# tests/test_agent_quality.py
import pytest
from agent import Agent
from evaluator import LLMJudge

@pytest.fixture
def agent():
    return Agent()

@pytest.fixture
def judge():
    return LLMJudge()

@pytest.mark.asyncio
async def test_relevance_threshold(agent, judge):
    """å›ç­”ã®é–¢é€£æ€§ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹ã“ã¨ã‚’ç¢ºèª"""
    test_cases = [
        {"question": "Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ", "expected_topic": "Python"},
        {"question": "æ©Ÿæ¢°å­¦ç¿’ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã¨ã¯ï¼Ÿ", "expected_topic": "ML"},
    ]

    for case in test_cases:
        response = await agent.run(case["question"])
        score = await judge.evaluate_relevance(case["question"], response)

        assert score >= 4, f"Relevance score {score} is below threshold for: {case['question']}"

@pytest.mark.asyncio
async def test_no_hallucination(agent, judge):
    """å¹»è¦šãŒãªã„ã“ã¨ã‚’ç¢ºèª"""
    # æ—¢çŸ¥ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è³ªå•
    context = "Pythonã¯1991å¹´ã«Guido van Rossumã«ã‚ˆã£ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚"
    question = "Pythonã¯ã„ã¤ä½œã‚‰ã‚Œã¾ã—ãŸã‹ï¼Ÿ"

    response = await agent.run(question, context=context)
    faithfulness = await judge.evaluate_faithfulness(response, context)

    assert faithfulness["judgment"] == "å¹»è¦šãªã—", f"Hallucination detected: {faithfulness}"
```

### GitHub Actionsçµ±åˆ

```yaml
# .github/workflows/agent-quality.yml
name: Agent Quality Check

on:
  pull_request:
    branches: [main]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run evaluation tests
        run: |
          pytest tests/test_agent_quality.py -v

      - name: Compare with baseline
        run: |
          python scripts/compare_baseline.py \
            --current-results results.json \
            --baseline baseline.json \
            --threshold 0.05

      - name: Upload results to Langfuse
        run: |
          python scripts/upload_evaluation.py \
            --results results.json \
            --commit ${{ github.sha }}
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. å…¨ã¦ã®LLMå‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹

```python
# âœ“ è‰¯ã„ä¾‹
@observe()
async def llm_call(prompt):
    return await llm.generate(prompt)

# âœ— æ‚ªã„ä¾‹
async def llm_call(prompt):
    return await llm.generate(prompt)  # ãƒˆãƒ¬ãƒ¼ã‚¹ãªã—
```

### 2. æ§‹é€ åŒ–ãƒ­ã‚°ã‚’ä½¿ç”¨

```python
# âœ“ è‰¯ã„ä¾‹
logger.info("tool_called", extra={
    "tool_name": "search",
    "args": {"query": query},
    "duration_ms": 150,
    "result_count": len(results),
})

# âœ— æ‚ªã„ä¾‹
logger.info(f"Called search with {query}, got {len(results)} results in 150ms")
```

### 3. ã‚³ã‚¹ãƒˆã‚’å¸¸ã«è¿½è·¡

```python
class CostTracker:
    PRICING = {
        "gpt-4o": {"input": 0.005, "output": 0.015},  # per 1K tokens
        "claude-3-opus": {"input": 0.015, "output": 0.075},
    }

    def calculate_cost(self, model, input_tokens, output_tokens):
        prices = self.PRICING.get(model, {"input": 0, "output": 0})
        return (
            (input_tokens / 1000) * prices["input"] +
            (output_tokens / 1000) * prices["output"]
        )
```

### 4. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æ´»ç”¨

```python
# å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è©•ä¾¡ã™ã‚‹ã®ã¯ã‚³ã‚¹ãƒˆé«˜
class SampledEvaluator:
    def __init__(self, sample_rate=0.1):
        self.sample_rate = sample_rate

    async def maybe_evaluate(self, interaction):
        if random.random() < self.sample_rate:
            return await self.evaluate(interaction)
        return None
```

---

## ã¾ã¨ã‚

### Observabilityãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
â–¡ ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
  - å…¨LLMå‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
  - å…¨ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
  - å…¥å‡ºåŠ›ã‚’è¨˜éŒ²ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼è€ƒæ…®ï¼‰

â–¡ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
  - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆP50, P90, P99ï¼‰
  - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
  - ã‚¨ãƒ©ãƒ¼ç‡
  - ã‚³ã‚¹ãƒˆ

â–¡ è©•ä¾¡
  - LLM-as-Judgeè¨­å®š
  - äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ãƒ­ãƒ¼
  - CI/CDçµ±åˆ

â–¡ ã‚¢ãƒ©ãƒ¼ãƒˆ
  - ç•°å¸¸æ¤œå‡ºãƒ«ãƒ¼ãƒ«
  - ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–
  - ã‚³ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆ

â–¡ ã‚¤ãƒ³ãƒ•ãƒ©
  - OpenTelemetryæº–æ‹ 
  - ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ é¸å®š
  - ãƒ‡ãƒ¼ã‚¿ä¿æŒãƒãƒªã‚·ãƒ¼
```

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [Langfuse Documentation](https://langfuse.com/docs)
- [OpenTelemetry for AI](https://opentelemetry.io/)
- [State of Agent Engineering (LangChain)](https://www.langchain.com/state-of-agent-engineering)
- [AI Observability Tools Guide (Braintrust)](https://www.braintrust.dev/articles/best-ai-observability-tools-2026)

---

**ã‚ãªãŸã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆObservabilityç’°å¢ƒã¯ã©ã†ãªã£ã¦ã„ã¾ã™ã‹ï¼Ÿå·¥å¤«ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚Œã°ã‚³ãƒ¡ãƒ³ãƒˆã§æ•™ãˆã¦ãã ã•ã„ï¼**
