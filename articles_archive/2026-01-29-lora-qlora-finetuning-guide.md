---
title: "ã€å®Ÿè·µã€‘LoRA/QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰ï½œGoogle Colabã®ç„¡æ–™GPUã§LLMã‚’èª¿æ•´ã™ã‚‹æ–¹æ³•"
emoji: "ğŸ¯"
type: "tech"
topics: ["lora", "llm", "æ©Ÿæ¢°å­¦ç¿’", "finetuning", "ai"]
published: false
---

**ã€ŒLLMã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã„ã‘ã©ã€GPUãŒé«˜ã™ãã‚‹...ã€**

A100ã‚’1æ™‚é–“ä½¿ã†ã¨$2ã€œ4ã€‚1æ—¥å›ã™ã¨$50ã€œ100ã€‚å€‹äººã«ã¯å³ã—ã„ã€‚

ã§ã‚‚ã€**LoRA**ã¨**QLoRA**ã‚’ä½¿ãˆã°ã€**Google Colabã®ç„¡æ–™T4 GPU**ã§ã‚‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚

## çµè«–ã‹ã‚‰è¨€ã†ã¨

:::message
- **LoRA**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®0.1%ã ã‘ã‚’å­¦ç¿’ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡1/10ã€‚
- **QLoRA**: ã•ã‚‰ã«4bité‡å­åŒ–ã§1/3ã«ã€‚T4ã§ã‚‚65Bãƒ¢ãƒ‡ãƒ«ãŒå‹•ãã€‚
- å“è³ªä½ä¸‹ã¯ã»ã¼ãªã—ã€‚ã‚³ã‚¹ãƒ‘æœ€å¼·ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã€‚
:::

## LoRAã¨ã¯ï¼Ÿ

### åŸºæœ¬æ¦‚å¿µ

**Low-Rank Adaptation**ï¼ˆä½ãƒ©ãƒ³ã‚¯é©å¿œï¼‰ã®ç•¥ã€‚

```
ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:
- å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
- 7Bãƒ¢ãƒ‡ãƒ«ã§28GB+ VRAMå¿…è¦
- ã‚³ã‚¹ãƒˆé«˜ã€æ™‚é–“ã‹ã‹ã‚‹

LoRA:
- è¿½åŠ ã®å°ã•ãªè¡Œåˆ—ã ã‘ã‚’å­¦ç¿’
- å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¯å‡çµï¼ˆå¤‰æ›´ã—ãªã„ï¼‰
- 7Bãƒ¢ãƒ‡ãƒ«ã§8GB VRAMã§å‹•ã
```

### æ•°å­¦çš„ãªä»•çµ„ã¿

```
å…ƒã®é‡ã¿è¡Œåˆ—: W (d Ã— k)
LoRAè¡Œåˆ—: A (d Ã— r) ã¨ B (r Ã— k)

æ›´æ–°å¾Œ: W' = W + BA

rï¼ˆãƒ©ãƒ³ã‚¯ï¼‰= 8ã€œ64ï¼ˆé€šå¸¸ï¼‰
â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒåŠ‡çš„ã«æ¸›å°‘
```

**ä¾‹**: 4096Ã—4096ã®è¡Œåˆ—
- ãƒ•ãƒ«: 16,777,216ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- LoRA (r=8): 65,536ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ**0.4%**ï¼‰

## QLoRAã¨ã¯ï¼Ÿ

### LoRA + é‡å­åŒ–

```
LoRA: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–
QLoRA: LoRA + 4bité‡å­åŒ– + ãƒšãƒ¼ã‚¸ãƒ‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶

ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:
ãƒ•ãƒ«: 100%
LoRA: 30%
QLoRA: 10%
```

### æŠ€è¡“çš„ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼

1. **4-bit NormalFloat**: æ–°ã—ã„é‡å­åŒ–å½¢å¼
2. **Double Quantization**: é‡å­åŒ–å®šæ•°ã‚‚é‡å­åŒ–
3. **Paged Optimizers**: ãƒ¡ãƒ¢ãƒªã‚¹ãƒ‘ã‚¤ã‚¯ã‚’é˜²ã

**çµæœ**: LLaMA 65Bã‚’**å˜ä¸€ã®48GB GPU**ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½

## å®Ÿè·µï¼šGoogle Colabã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### ç’°å¢ƒè¨­å®š

```python
!pip install -q transformers peft bitsandbytes accelerate datasets trl

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
```

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆQLoRAç”¨4bitï¼‰

```python
# 4bité‡å­åŒ–è¨­å®š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # äºŒé‡é‡å­åŒ–
    bnb_4bit_quant_type="nf4",       # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
```

### LoRAè¨­å®š

```python
# LoRAè¨­å®š
lora_config = LoraConfig(
    r=16,                      # ãƒ©ãƒ³ã‚¯ï¼ˆ8-64ãŒä¸€èˆ¬çš„ï¼‰
    lora_alpha=32,             # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
    target_modules=[           # é©ç”¨ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"       # FFN
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# ãƒ¢ãƒ‡ãƒ«ã«LoRAã‚’é©ç”¨
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª
model.print_trainable_parameters()
# å‡ºåŠ›ä¾‹: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™

```python
from datasets import load_dataset

# ä¾‹ï¼šæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
dataset = load_dataset("kunishou/databricks-dolly-15k-ja")

def format_instruction(sample):
    return f"""### æŒ‡ç¤º:
{sample['instruction']}

### å…¥åŠ›:
{sample['input']}

### å›ç­”:
{sample['output']}"""

# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨
dataset = dataset.map(lambda x: {"text": format_instruction(x)})
```

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./lora-output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    optim="paged_adamw_8bit",  # ãƒšãƒ¼ã‚¸ãƒ‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    args=training_args,
    tokenizer=tokenizer,
    max_seq_length=512,
    dataset_text_field="text",
)

# å­¦ç¿’é–‹å§‹
trainer.train()
```

### ä¿å­˜ã¨èª­ã¿è¾¼ã¿

```python
# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ä¿å­˜ï¼ˆæ•°åMBï¼‰
model.save_pretrained("./my-lora-adapter")

# æ¨è«–æ™‚ã®ãƒ­ãƒ¼ãƒ‰
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, "./my-lora-adapter")
```

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

### ãƒ©ãƒ³ã‚¯ï¼ˆrï¼‰ã®é¸ã³æ–¹

| r | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ç”¨é€” |
|---|-------------|------|
| 4 | æœ€å° | ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ |
| 8 | å° | ä¸€èˆ¬çš„ãªå¾®èª¿æ•´ |
| 16 | ä¸­ | **æ¨å¥¨**ï¼ˆãƒãƒ©ãƒ³ã‚¹è‰¯ã„ï¼‰ |
| 32 | å¤§ | è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ |
| 64 | æœ€å¤§ | å¤§è¦æ¨¡ãªé©å¿œ |

### alphaå€¤ã®è¨­å®š

```
æ¨å¥¨: alpha = 2 Ã— r

ä¾‹:
r=16 â†’ alpha=32
r=32 â†’ alpha=64
```

### target_modules

```python
# æœ€å°æ§‹æˆï¼ˆAttentionã®ã¿ï¼‰
target_modules = ["q_proj", "v_proj"]

# æ¨å¥¨æ§‹æˆï¼ˆå…¨Linearå±¤ï¼‰
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]

# ç ”ç©¶çµæœ: å…¨Linearå±¤ã‚’å¯¾è±¡ã«ã™ã‚‹ã¨ç²¾åº¦å‘ä¸Š
```

## LoRA vs QLoRAï¼šã©ã¡ã‚‰ã‚’ä½¿ã†ï¼Ÿ

| æ¡ä»¶ | æ¨å¥¨ |
|------|------|
| VRAM 24GBä»¥ä¸Š | LoRA |
| VRAM 16GBä»¥ä¸‹ | **QLoRA** |
| å­¦ç¿’é€Ÿåº¦é‡è¦– | LoRA |
| ãƒ¡ãƒ¢ãƒªåˆ¶ç´„å³ã—ã„ | QLoRA |
| å“è³ªæœ€å„ªå…ˆ | LoRA |

### é€Ÿåº¦æ¯”è¼ƒ

```
LoRA:   100%ï¼ˆåŸºæº–ï¼‰
QLoRA:  139%ï¼ˆ39%é…ã„ï¼‰

ç†ç”±: é‡å­åŒ–/é€†é‡å­åŒ–ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
```

### å“è³ªæ¯”è¼ƒ

```
ç ”ç©¶çµæœ: QLoRAã¨LoRAã®å“è³ªå·®ã¯ã€Œç„¡è¦–ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã€
```

## 2026å¹´æœ€æ–°ï¼šLoRAFusion

4æœˆã®EUROSYS '26ã§ç™ºè¡¨äºˆå®šã®æ–°æŠ€è¡“ã€‚

**æ”¹å–„ç‚¹**:
- ã‚«ãƒ¼ãƒãƒ«ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–
- ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã®åŠ¹ç‡åŒ–
- QLoRAã«ã‚‚é©ç”¨å¯èƒ½

**åŠ¹æœ**:
- å­¦ç¿’é€Ÿåº¦2å€
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã•ã‚‰ã«å‘ä¸Š

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

### CUDA Out of Memory

```python
# è§£æ±ºç­–1: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä¸‹ã’ã‚‹
per_device_train_batch_size=2
gradient_accumulation_steps=8

# è§£æ±ºç­–2: max_seq_lengthã‚’ä¸‹ã’ã‚‹
max_seq_length=256

# è§£æ±ºç­–3: gradient_checkpointingã‚’æœ‰åŠ¹åŒ–
model.gradient_checkpointing_enable()
```

### éå­¦ç¿’

```python
# è§£æ±ºç­–: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä¸Šã’ã‚‹
lora_dropout=0.1

# ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã™
num_train_epochs=1
```

## ã¾ã¨ã‚

| é …ç›® | ãƒ•ãƒ«FT | LoRA | QLoRA |
|------|--------|------|-------|
| VRAMï¼ˆ7Bï¼‰ | 28GB+ | 8GB | **6GB** |
| å­¦ç¿’æ™‚é–“ | 100% | 30% | 42% |
| å“è³ª | 100% | 99% | 98% |
| ã‚³ã‚¹ãƒˆ | $$$ | $ | **$** |

:::message
2026å¹´ã€LLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã¯**LoRA/QLoRA**ã€‚ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ç ”ç©¶æ©Ÿé–¢ã§ã‚‚æ¸›å°‘å‚¾å‘ã€‚å€‹äººé–‹ç™ºè€…ã§ã‚‚é«˜å“è³ªãªã‚«ã‚¹ã‚¿ãƒ LLMã‚’ä½œã‚Œã‚‹æ™‚ä»£ã§ã™ã€‚
:::

---

ã“ã®è¨˜äº‹ãŒå½¹ã«ç«‹ã£ãŸã‚‰ã€**ã„ã„ã­**ã¨**ä¿å­˜**ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼

**è³ªå•**: LoRA/QLoRAã§ã©ã‚“ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚Šã¾ã—ãŸã‹ï¼Ÿã‚³ãƒ¡ãƒ³ãƒˆã§æ•™ãˆã¦ãã ã•ã„ï¼

## å‚è€ƒãƒªãƒ³ã‚¯

- [LoRA vs QLoRA - Modal](https://modal.com/blog/lora-qlora)
- [Practical Tips for LoRA - Sebastian Raschka](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
- [LoRAFusion Paper - arXiv](https://arxiv.org/html/2510.00206v1)
- [Efficient Fine-Tuning Guide - Databricks](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms)
