---
title: "ã€å®Œå…¨è§£èª¬ã€‘JitRLï¼šå‹¾é…æ›´æ–°ãªã—ã§LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç¶™ç¶šå­¦ç¿’ã•ã›ã‚‹é©å‘½çš„æ‰‹æ³•ã®ã‚³ãƒ¼ãƒ‰è§£æ"
emoji: "ğŸ§ª"
type: "tech"
topics: ["ai", "llm", "å¼·åŒ–å­¦ç¿’", "ç¶™ç¶šå­¦ç¿’", "python"]
published: true
---

**ã€ŒLLMã‚’å­¦ç¿’ã•ã›ãšã«ã€å­¦ç¿’ã•ã›ã‚‹ã€**

ã“ã‚Œã¯çŸ›ç›¾ã—ã¦ã„ã‚‹ã‚ˆã†ã«èã“ãˆã‚‹ãŒã€2026å¹´1æœˆ26æ—¥ã«å…¬é–‹ã•ã‚ŒãŸ[JitRLï¼ˆJust-In-Time Reinforcement Learningï¼‰](https://github.com/liushiliushi/JitRL)ã¯ã¾ã•ã«ã“ã‚Œã‚’å®Ÿç¾ã—ãŸã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€JitRLã®GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’å®Œå…¨è§£æã—ã€**Policy Tripletã¨ã¯ä½•ã‹**ã€**ã©ã®ã‚ˆã†ã«ã—ã¦å‹¾é…æ›´æ–°ãªã—ã§ç¶™ç¶šå­¦ç¿’ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã®ã‹**ã‚’ã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§æ·±æ˜ã‚Šã™ã‚‹ã€‚

## çµè«–ã‹ã‚‰è¨€ã†ã¨

:::message
**JitRL** = LLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€åˆ‡æ›´æ–°ã›ãšã«ã€éå»ã®çµŒé¨“ã‚’**éãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¡ãƒ¢ãƒª**ã«ä¿å­˜ã—ã€æ¨è«–æ™‚ã«é¡ä¼¼çµŒé¨“ã‚’æ¤œç´¢ã—ã¦**ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå‡ºåŠ›ç¢ºç‡ï¼‰ã‚’èª¿æ•´**ã™ã‚‹ã“ã¨ã§ã€ç¶™ç¶šå­¦ç¿’ã‚’å®Ÿç¾ã™ã‚‹æ‰‹æ³•ã€‚

**Policy Triplet** = `<state, action, reward>` ã®3ã¤çµ„ã€‚ã“ã‚Œã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã€é¡ä¼¼çŠ¶æ³ã§éå»ã®action-rewardã‚’å‚ç…§ã—ã¦ãƒãƒªã‚·ãƒ¼ã‚’æ”¹å–„ã™ã‚‹ã€‚
:::

---

## 1. å¾“æ¥ã®å¼·åŒ–å­¦ç¿’ vs JitRLï¼šä½•ãŒé•ã†ã®ã‹ï¼Ÿ

### å¾“æ¥ã®RLï¼ˆReinforcement Learningï¼‰

```
çµŒé¨“åé›† â†’ å‹¾é…è¨ˆç®— â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° â†’ å¤ã„çŸ¥è­˜ãŒå£Šã‚Œã‚‹ï¼ˆç ´æ»…çš„å¿˜å´ï¼‰
```

### JitRL

```
çµŒé¨“åé›† â†’ ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ â†’ æ¨è«–æ™‚ã«æ¤œç´¢ â†’ ãƒ­ã‚¸ãƒƒãƒˆèª¿æ•´ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¤‰ã‚ã‚‰ãªã„
```

**æ±ºå®šçš„ãªé•ã„**ï¼šJitRLã¯LLMã®é‡ã¿ã‚’**ä¸€åˆ‡å¤‰æ›´ã—ãªã„**ã€‚ä»£ã‚ã‚Šã«ã€éå»ã®çµŒé¨“ã‚’å¤–éƒ¨ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã€æ¨è«–æ™‚ã«ãã®ãƒ¡ãƒ¢ãƒªã‹ã‚‰é–¢é€£ã™ã‚‹çµŒé¨“ã‚’æ¤œç´¢ã—ã¦ã€å‡ºåŠ›ã®ç¢ºç‡åˆ†å¸ƒã‚’èª¿æ•´ã™ã‚‹ã€‚

---

## 2. Policy Tripletã¨ã¯ä½•ã‹ï¼Ÿ

### æ•°å­¦çš„å®šç¾©

Policy Tripletã¯å¼·åŒ–å­¦ç¿’ã®åŸºæœ¬å˜ä½ã§ã€ä»¥ä¸‹ã®3è¦ç´ ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ï¼š

```python
triplet = {
    'state': str,      # ç¾åœ¨ã®çŠ¶æ…‹ï¼ˆç’°å¢ƒã®è¦³æ¸¬ï¼‰
    'action': str,     # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå–ã£ãŸè¡Œå‹•
    'reward': float    # ãã®è¡Œå‹•ã«å¯¾ã™ã‚‹å ±é…¬
}
```

JitRLã§ã¯ã€ã“ã‚Œã‚’æ‹¡å¼µã—ãŸ**Step Metadata**ã¨ã—ã¦ä¿å­˜ã™ã‚‹ï¼š

```python
# cross_episode_memory.py ã‚ˆã‚Š
step_metadata = {
    'episode_timestamp': episode_data.get('timestamp'),
    'episode_number': self.current_episode_number,
    'step_data': step_data.copy(),  # state, action, rewardã‚’å«ã‚€
    'episode_final_score': episode_data.get('final_score'),
    'episode_success': episode_data.get('success'),
    'trajectory_context': trajectory_context,  # å±¥æ­´ã‚µãƒãƒªãƒ¼
    'current_env_info': current_env_info,      # ç¾åœ¨ã®ç’°å¢ƒæƒ…å ±
    'future_rewards': future_rewards           # å°†æ¥ã®å ±é…¬ç³»åˆ—ï¼ˆå‰²å¼•è¨ˆç®—ç”¨ï¼‰
}
```

### ãªãœTripletãŒé‡è¦ãªã®ã‹ï¼Ÿ

Policy Tripletã¯ã€Œ**ã“ã®çŠ¶æ…‹ã§ã€ã“ã®è¡Œå‹•ã‚’å–ã£ãŸã‚‰ã€ã“ã®å ±é…¬ãŒå¾—ã‚‰ã‚ŒãŸ**ã€ã¨ã„ã†çµŒé¨“ã®è¨˜éŒ²ã ã€‚

å¾“æ¥ã®RLã§ã¯ã“ã®çµŒé¨“ã‚’ä½¿ã£ã¦**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°**ã™ã‚‹ãŒã€JitRLã§ã¯**ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã¦æ¨è«–æ™‚ã«å‚ç…§**ã™ã‚‹ã€‚

---

## 3. JitRLã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³è§£

### ãƒªãƒã‚¸ãƒˆãƒªæ§‹é€ 

```
JitRL/
â”œâ”€â”€ Jericho/                          # ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã‚²ãƒ¼ãƒ ç”¨
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ memory_agent.py           # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆ850è¡Œï¼‰
â”‚       â”œâ”€â”€ cross_episode_memory.py   # ã‚³ã‚¢ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ ï¼ˆ900è¡Œï¼‰
â”‚       â””â”€â”€ utils.py                  # LLMãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ1150è¡Œï¼‰
â”‚
â””â”€â”€ WebArena/                         # Webè‡ªå‹•åŒ–ç”¨
    â””â”€â”€ memory_agents/
        â””â”€â”€ utils/
            â””â”€â”€ cross_episode_memory.py  # ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ ï¼ˆ1200è¡Œï¼‰
```

### æ ¸å¿ƒï¼šCrossEpisodeMemoryã‚¯ãƒ©ã‚¹

```python
class CrossEpisodeMemory:
    def __init__(self, base_dir: str, gamma: float = 0.95, ...):
        self.gamma = gamma  # å‰²å¼•ç‡

        # ãƒ‡ãƒ¥ã‚¢ãƒ«Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå±¥æ­´ + çŠ¶æ…‹ï¼‰
        self.history_index = faiss.IndexFlatIP(self.vector_dim)
        self.state_index = faiss.IndexFlatIP(self.vector_dim)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.step_metadata = []
```

**ãƒã‚¤ãƒ³ãƒˆ**ï¼š
- **ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**ï¼šå±¥æ­´ãƒ™ã‚¯ãƒˆãƒ«ã¨ç¾åœ¨çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’åˆ¥ã€…ã«ç®¡ç†
- **Faiss**ï¼šé«˜é€Ÿãªé¡ä¼¼åº¦æ¤œç´¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **gammaï¼ˆå‰²å¼•ç‡ï¼‰**ï¼šå°†æ¥ã®å ±é…¬ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹

---

## 4. çµŒé¨“ã®ä¿å­˜ï¼šTripletã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã¸

### ã‚¹ãƒ†ãƒƒãƒ—1ï¼šçŠ¶æ…‹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰

```python
def _encode_trajectory_context(self, states, actions, current_step, current_summary, info=None):
    # LLMã§å±¥æ­´ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
    trajectory_context, current_env_info = generate_trajectory_context_for_vector(
        states=states,
        actions=actions,
        earlier_summary=earlier_summary,
        current_step=current_step,
        episode_number=self.current_episode_number,
        llm_model=self.llm_model,
        temperature=0.8,
        max_tokens=1000,
        info=info
    )

    # å±¥æ­´ã‚µãƒãƒªãƒ¼ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆOpenAI Embeddingï¼‰
    history_vector = get_embedding_with_retries(current_summary, model=self.embedding_model)

    # ç¾åœ¨çŠ¶æ…‹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    current_state_text = f"step {current_step}: State: {current_env_info}"
    state_vector = get_embedding_with_retries(current_state_text, model=self.embedding_model)

    # æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ç”¨ï¼‰
    history_vector = history_vector / (np.linalg.norm(history_vector) + 1e-8)
    state_vector = state_vector / (np.linalg.norm(state_vector) + 1e-8)

    return trajectory_context, current_env_info, history_vector, state_vector
```

### ã‚¹ãƒ†ãƒƒãƒ—2ï¼šãƒ¡ãƒ¢ãƒªã¸ã®ä¿å­˜

```python
def _store_step_in_vector_db(self, states, actions, step_index, episode_data):
    # å°†æ¥ã®å ±é…¬ç³»åˆ—ã‚’æŠ½å‡ºï¼ˆå‰²å¼•å ±é…¬è¨ˆç®—ç”¨ï¼‰
    future_rewards = []
    for u in range(step_index, len(steps)):
        future_rewards.append(steps[u].get('llm_step_score', 0))

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    step_metadata = {
        'step_data': step_data.copy(),
        'trajectory_context': trajectory_context,
        'current_env_info': current_env_info,
        'future_rewards': future_rewards  # â˜…é‡è¦ï¼šå‰²å¼•å ±é…¬è¨ˆç®—ã«ä½¿ç”¨
    }

    # ãƒ‡ãƒ¥ã‚¢ãƒ«Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
    self.history_index.add(history_vector.reshape(1, -1))
    self.state_index.add(state_vector.reshape(1, -1))
    self.step_metadata.append(step_metadata)
```

---

## 5. çµŒé¨“ã®æ¤œç´¢ï¼šé¡ä¼¼çŠ¶æ³ã®ç™ºè¦‹

### ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢

```python
def retrieve_similar_with_vector(self, game_history, current_state, current_summary, k=3, r=0.7, info=None):
    # ç¾åœ¨ã®çŠ¶æ…‹ã‚’ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    trajectory_context, current_env_info, query_history_vec, query_state_vec = \
        self._encode_trajectory_context(query_states, query_actions, len(query_states)-1, current_summary, info)

    # ä¸¡æ–¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§æ¤œç´¢
    recall_size = min(max(k * 10, 100), self.history_index.ntotal)
    history_scores, history_indices = self.history_index.search(query_history_vec.reshape(1,-1), recall_size)
    state_scores, state_indices = self.state_index.search(query_state_vec.reshape(1,-1), recall_size)
```

### Jaccardé¡ä¼¼åº¦ã«ã‚ˆã‚‹ç²¾ç·»åŒ–

ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã ã‘ã§ãªãã€**N-gram Jaccardé¡ä¼¼åº¦**ã‚‚ä½¿ç”¨ï¼š

```python
def _jaccard(self, a_tokens, b_tokens, ngram=3):
    # N-gramã‚’ç”Ÿæˆ
    a_ngrams = self._get_ngrams(a_tokens, ngram)
    b_ngrams = self._get_ngrams(b_tokens, ngram)

    # å¤šé‡é›†åˆã§Jaccardä¿‚æ•°ã‚’è¨ˆç®—
    from collections import Counter
    counter_a = Counter(a_ngrams)
    counter_b = Counter(b_ngrams)

    inter = sum((counter_a & counter_b).values())
    union = sum((counter_a | counter_b).values())

    return inter / union if union > 0 else 0.0
```

### æœ€çµ‚é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢

```python
# å±¥æ­´é¡ä¼¼åº¦ï¼ˆ1-gramï¼‰ã¨çŠ¶æ…‹é¡ä¼¼åº¦ï¼ˆ4-gramï¼‰ã®åŠ é‡å¹³å‡
sim1 = self._jaccard(query_history_tokens, history_tokens, ngram=1)
sim2 = self._jaccard(query_current_tokens, current_tokens, ngram=4)
similarity = sim1 * 0.3 + sim2 * 0.7  # ç¾åœ¨çŠ¶æ…‹ã‚’é‡è¦–
```

---

## 6. ãƒ­ã‚¸ãƒƒãƒˆèª¿æ•´ï¼šJitRLã®æ ¸å¿ƒ

### Advantageï¼ˆã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰ã®è¨ˆç®—

æ¤œç´¢ã§è¦‹ã¤ã‹ã£ãŸé¡ä¼¼çµŒé¨“ã‹ã‚‰ã€å„è¡Œå‹•ã®**å‰²å¼•å ±é…¬**ã‚’è¨ˆç®—ï¼š

```python
# memory_agent.py ã‚ˆã‚Š
def update_scores(self, state_node, options_with_logits, k, r, memory_text, info=None):
    nearest_trajectories = self.cross_mem.retrieve_similar(...)

    # è¡Œå‹•ã”ã¨ã®å ±é…¬ã‚’é›†è¨ˆ
    action_rewards = {}
    for sim, discounted_return, result_dict in nearest_trajectories:
        action = result_dict.get('action', '').strip()
        discounted_reward = result_dict.get('discounted_reward', 0)
        if action not in action_rewards:
            action_rewards[action] = []
        action_rewards[action].append(discounted_reward)

    # è¡Œå‹•ã”ã¨ã®å¹³å‡å ±é…¬
    action_avg_rewards = {}
    for action, rewards in action_rewards.items():
        action_avg_rewards[action] = sum(rewards) / len(rewards)

    # å…¨ä½“ã®å¹³å‡å ±é…¬ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
    all_rewards = []
    for rewards_list in action_rewards.values():
        all_rewards.extend(rewards_list)
    overall_avg_reward = sum(all_rewards) / len(all_rewards)

    # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ = è¡Œå‹•ã®å ±é…¬ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    action_advantages = {}
    for action, avg_reward in action_avg_rewards.items():
        action_advantages[action] = avg_reward - overall_avg_reward
```

### ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã®æ­£è¦åŒ–

```python
# æ­£ã®ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã§æ­£è¦åŒ–
positive_advs = [adv for adv in adv_values if adv > 0]
if positive_advs:
    max_positive = max(positive_advs)
    normalized_advantages = {
        action: adv / max_positive
        for action, adv in action_advantages.items()
    }
```

### ãƒ­ã‚¸ãƒƒãƒˆã®èª¿æ•´ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰

```python
# ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ä»˜ã‘ï¼ˆå­¦ç¿’ãŒé€²ã‚€ã»ã©çµŒé¨“ã‚’é‡è¦–ï¼‰
current_episode = self.cross_mem.current_episode_number
episode_weight = min(1.0 + (current_episode / 50.0) * 0.5, 1.5)

# é‡ã¿ä»˜ãæ­£è¦åŒ–ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸
weighted_normalized_advantage = normalized_advantage * episode_weight

# â˜…â˜…â˜… ãƒ­ã‚¸ãƒƒãƒˆèª¿æ•´ â˜…â˜…â˜…
# LLMã®å‡ºåŠ›ç¢ºç‡ã«ã€éå»ã®çµŒé¨“ã‹ã‚‰è¨ˆç®—ã—ãŸã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’åŠ ç®—
corrected_logprob = normalized_prob + weighted_normalized_advantage
```

:::message alert
**ã“ã‚ŒãŒJitRLã®æ ¸å¿ƒ**ï¼šLLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¤‰ãˆãšã«ã€å‡ºåŠ›ã®ãƒ­ã‚°ç¢ºç‡ï¼ˆlogitï¼‰ã«çµŒé¨“ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’**åŠ ç®—**ã™ã‚‹ã“ã¨ã§ã€ãƒãƒªã‚·ãƒ¼ã‚’æ”¹å–„ã™ã‚‹ã€‚
:::

---

## 7. æ•°å­¦çš„èƒŒæ™¯ï¼šãªãœã“ã‚ŒãŒæ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ

è«–æ–‡ã«ã‚ˆã‚‹ã¨ã€ã“ã®ãƒ­ã‚¸ãƒƒãƒˆåŠ ç®—ãƒ«ãƒ¼ãƒ«ã¯**KLåˆ¶ç´„ä»˜ããƒãƒªã‚·ãƒ¼æœ€é©åŒ–å•é¡Œã®é–‰å½¢å¼è§£**ã§ã‚ã‚‹ã€‚

### æœ€é©åŒ–å•é¡Œ

```
max_Ï€ E[A(s,a)] - Î² Ã— KL(Ï€ || Ï€_ref)
```

- `Ï€`: æ–°ã—ã„ãƒãƒªã‚·ãƒ¼
- `Ï€_ref`: å‚ç…§ãƒãƒªã‚·ãƒ¼ï¼ˆå…ƒã®LLMï¼‰
- `A(s,a)`: ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°
- `Î²`: KLåˆ¶ç´„ã®å¼·ã•

### é–‰å½¢å¼è§£

```
log Ï€(a|s) = log Ï€_ref(a|s) + A(s,a) / Î²
```

ã“ã‚Œã¯**ã‚½ãƒ•ãƒˆmaxæ–¹ç¨‹å¼**ã®ä¸€ç¨®ã§ã€JitRLã®ãƒ­ã‚¸ãƒƒãƒˆåŠ ç®—ï¼š

```python
corrected_logprob = normalized_prob + weighted_normalized_advantage
```

ã¨æ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹ã€‚

---

## 8. æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹

### é©å¿œçš„æ¢ç´¢ç¢ºç‡

```python
def calculate_exploration_probability(self, nearest_trajectories, action_rewards):
    """
    å®‰å®šã—ãŸé«˜å ±é…¬è¡Œå‹•ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã§æ¢ç´¢ç¢ºç‡ã‚’èª¿æ•´
    """
    for action, rewards in action_rewards.items():
        n_samples = len(rewards)
        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)

        # å¤‰å‹•ä¿‚æ•°ï¼ˆCVï¼‰: ç›¸å¯¾çš„ãªå®‰å®šæ€§ã‚’æ¸¬å®š
        cv = std_reward / mean_reward if mean_reward > 0 else float('inf')

        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        performance_score = max(0, mean_reward)
        stability_score = 1.0 / (1.0 + cv)
        sample_confidence = max(n_samples / 4.0, 1)

        action_confidence = (performance_score * stability_score) ** self.exploration_rate * sample_confidence

    # æ¢ç´¢ç¢ºç‡ = ä¿¡é ¼åº¦ã®é€†æ•°
    exploration_prob = 1 / (1 + max_confidence)
    return max(0.05, min(0.8, exploration_prob))  # 0.05ã€œ0.8ã«ã‚¯ãƒ©ãƒ³ãƒ—
```

**ãƒã‚¤ãƒ³ãƒˆ**ï¼š
- **å®‰å®šã—ãŸé«˜å ±é…¬è¡Œå‹•ãŒã‚ã‚‹** â†’ æ¢ç´¢ã‚’æ¸›ã‚‰ã—ã¦æ´»ç”¨
- **ä¸ç¢ºå®ŸãªçŠ¶æ³** â†’ æ¢ç´¢ã‚’å¢—ã‚„ã—ã¦æ–°ã—ã„è¡Œå‹•ã‚’è©¦ã™

---

## 9. LLMã«ã‚ˆã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

JitRLã§ã¯ã€ã‚²ãƒ¼ãƒ ã®å ±é…¬ã ã‘ã§ãªãã€**LLMã«ã‚ˆã‚‹è¡Œå‹•è©•ä¾¡ã‚¹ã‚³ã‚¢**ã‚‚ä½¿ç”¨ï¼š

```python
def evaluate_step_scores_with_llm(game_history, state, final_score, success, llm_model):
    sys_prompt = """You are scoring game actions to build training data for future gameplay.

    SCORING RULES:
    - Positive: Action led to progress or useful discoveries
    - Negative: Action wasted time, caused loops, or had no benefit
    """

    user_prompt = f"""Score each action in this game session.

    Final Result: {"SUCCESS" if success else "FAILURE"}, Final Score: {final_score}

    Trajectory:
    {trajectory_text}

    JSON FORMAT:
    {{
      "step_analysis": [
        {{
          "step": 0,
          "action": "exact action taken",
          "detailed_reasoning": "What happened after this action...",
          "score": 5,
        }}
      ]
    }}
    """
```

ã“ã‚Œã«ã‚ˆã‚Šã€**ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆã¾ã°ã‚‰ï¼‰ãªç’°å¢ƒå ±é…¬**ã‚’è£œå®Œã™ã‚‹**å¯†ãªå ±é…¬ä¿¡å·**ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

---

## 10. å®Ÿé¨“çµæœ

### Jerichoãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼

| ã‚²ãƒ¼ãƒ  | ãƒŠã‚¤ãƒ¼ãƒ– | JitRL | æ”¹å–„ç‡ |
|--------|----------|-------|--------|
| Zork1 | 35.2 | 52.8 | **+50%** |
| Library | 18.5 | 28.3 | **+53%** |
| Detective | 180 | 265 | **+47%** |

### WebArena-Liteï¼ˆWebè‡ªå‹•åŒ–ï¼‰

| ãƒ¢ãƒ‡ãƒ« | ãƒ¡ãƒ¢ãƒªãªã— | JitRL | æ”¹å–„ç‡ |
|--------|------------|-------|--------|
| GPT-4o | 32.5% | 41.2% | **+27%** |
| Claude-3.5 | 28.8% | 38.5% | **+34%** |
| Gemini-2.5 | 25.3% | 34.7% | **+37%** |

---

## 11. JitRLã‚’è‡ªåˆ†ã§è©¦ã™

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
git clone https://github.com/liushiliushi/JitRL.git
cd JitRL/Jericho

pip install jericho openai tiktoken numpy python-dotenv faiss-cpu
```

### å®Ÿè¡Œ

```bash
# Zork1ã§10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
python main.py --game_name zork1 --agent_type memory --eval_runs 10

# ãƒ¡ãƒ¢ãƒªãªã—ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
python main.py --game_name zork1 --agent_type memory --no-enable_cross_mem
```

### ç’°å¢ƒè¨­å®š

```bash
# .env
OPENROUTER_API_KEY=your_api_key
```

---

## 12. ã¾ã¨ã‚ï¼šJitRLãŒé–‹ãæœªæ¥

### JitRLã®é©æ–°æ€§

1. **å‹¾é…æ›´æ–°ãªã—**ï¼šLLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€åˆ‡å¤‰æ›´ã—ãªã„
2. **ãƒ†ã‚¹ãƒˆæ™‚å­¦ç¿’**ï¼šæ¨è«–æ™‚ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒãƒªã‚·ãƒ¼ã‚’æ”¹å–„
3. **ç ´æ»…çš„å¿˜å´ã®å›é¿**ï¼šçµŒé¨“ã¯å¤–éƒ¨ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã•ã‚Œã‚‹ãŸã‚ã€çŸ¥è­˜ãŒå¤±ã‚ã‚Œãªã„
4. **æ•°å­¦çš„æ ¹æ‹ **ï¼šKLåˆ¶ç´„ä»˜ãæœ€é©åŒ–ã®é–‰å½¢å¼è§£

### ä»Šå¾Œã®å¯èƒ½æ€§

- **Claude Codeã‚„Cursorã¸ã®å¿œç”¨**ï¼šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®çŸ¥è­˜ã‚’è“„ç©
- **ä¼æ¥­å‘ã‘AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**ï¼šé¡§å®¢å¯¾å¿œã®çµŒé¨“ã‚’è“„ç©ã—ã¦å“è³ªå‘ä¸Š
- **è‡ªå‹•é‹è»¢**ï¼šç•°å¸¸çŠ¶æ³ã¸ã®å¯¾å¿œçµŒé¨“ã‚’è“„ç©

---

:::message
ã“ã®è¨˜äº‹ãŒå‚è€ƒã«ãªã£ãŸã‚‰ã€**ã„ã„ã­**ã¨**ä¿å­˜**ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼

**è³ªå•**ï¼šã‚ãªãŸã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§JitRLã®ã‚ˆã†ãªçµŒé¨“è“„ç©ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã£ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã‹ï¼Ÿã©ã‚“ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã¦ã„ã¾ã™ã‹ï¼Ÿã‚³ãƒ¡ãƒ³ãƒˆã§æ•™ãˆã¦ãã ã•ã„ã€‚
:::

## å‚è€ƒæ–‡çŒ®

- [JitRL GitHub Repository](https://github.com/liushiliushi/JitRL)
- [JitRL Paper (arXiv:2601.18510)](https://www.arxiv.org/pdf/2601.18510)
- [Jericho: Text Adventure Game Framework](https://github.com/microsoft/jericho)
- [WebArena Benchmark](https://webarena.dev/)
- [Faiss: Vector Similarity Search](https://github.com/facebookresearch/faiss)
