---
title: "ã€2026å¹´æ±ºå®šç‰ˆã€‘RAGãƒ„ãƒ¼ãƒ«45é¸ã‚’å®Ÿè£…ã—ã¦æ¯”è¼ƒã—ãŸ - é¸å®šåŸºæº–ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å®Œå…¨ã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ”¥"
type: "tech"
topics: ["rag", "llm", "ai", "langchain", "llamaindex"]
published: false
---

## ã“ã®è¨˜äº‹ã§å¾—ã‚‰ã‚Œã‚‹ã“ã¨

- RAGãƒ„ãƒ¼ãƒ«é¸å®šã®**å…·ä½“çš„ãªåˆ¤æ–­åŸºæº–**
- å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã¨**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ**
- ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥ã®**æ¨å¥¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**
- æœ¬ç•ªé‹ç”¨ã§é­é‡ã™ã‚‹**è½ã¨ã—ç©´ã¨å¯¾ç­–**

å…¨ãƒ„ãƒ¼ãƒ«ã‚’ã‚«ã‚¿ãƒ­ã‚°åŒ–ã—ã¾ã—ãŸï¼š
https://rag-catalog.vercel.app

:::message alert
ã“ã®è¨˜äº‹ã¯å®Ÿéš›ã«45ä»¥ä¸Šã®ãƒ„ãƒ¼ãƒ«ã‚’æ¤œè¨¼ã—ãŸçµæœã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚
:::

---

## RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å…¨ä½“åƒ

ã¾ãšã€RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆè¦ç´ ã‚’æ•´ç†ã—ã¾ã™ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Ingest  â”‚ â†’ â”‚  Index   â”‚ â†’ â”‚ Retrieve â”‚ â†’ â”‚ Generate â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚              â”‚              â”‚              â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Docling â”‚   â”‚ Chroma  â”‚   â”‚ Hybrid  â”‚   â”‚ LLM     â”‚    â”‚
â”‚  â”‚ Unstructâ”‚   â”‚ Qdrant  â”‚   â”‚ Rerank  â”‚   â”‚ + Eval  â”‚    â”‚
â”‚  â”‚ Firecrawl   â”‚ Milvus  â”‚   â”‚ ColBERT â”‚   â”‚ RAGAS   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æœ€é©ãªãƒ„ãƒ¼ãƒ«ãŒç•°ãªã‚Šã¾ã™ã€‚

---

## ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¯”è¼ƒï¼šLangChain vs LlamaIndex vs Haystack

### å®šé‡æ¯”è¼ƒ

| æŒ‡æ¨™ | LangChain | LlamaIndex | Haystack |
|------|-----------|------------|----------|
| GitHub Stars | 105k | 38k | 18k |
| é€±é–“DLæ•° | 2.8M | 980k | 320k |
| åˆå›ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | 15åˆ† | 10åˆ† | 20åˆ† |
| ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢é€Ÿåº¦* | 42ms | 38ms | 45ms |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡* | 1.2GB | 890MB | 1.1GB |

*10ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€text-embedding-3-smallä½¿ç”¨æ™‚

### ã‚³ãƒ¼ãƒ‰æ¯”è¼ƒï¼šåŒã˜RAGã‚’3ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§å®Ÿè£…

**LangChain**
```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
loader = DirectoryLoader("./docs", glob="**/*.md")
documents = loader.load()

# ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())

# RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o"),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True
)

result = qa.invoke("RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ")
```

**LlamaIndex**
```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ + ãƒ‘ãƒ¼ã‚¹
documents = SimpleDirectoryReader("./docs").load_data()
parser = SentenceSplitter(chunk_size=1000, chunk_overlap=200)
nodes = parser.get_nodes_from_documents(documents)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
index = VectorStoreIndex(nodes)

# ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
query_engine = index.as_query_engine(
    llm=OpenAI(model="gpt-4o"),
    similarity_top_k=5
)

response = query_engine.query("RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ")
```

**Haystack**
```python
from haystack import Pipeline
from haystack.components.converters import MarkdownToDocument
from haystack.components.preprocessors import DocumentSplitter
from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder
from haystack.components.writers import DocumentWriter
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.generators import OpenAIGenerator
from haystack.components.builders import PromptBuilder
from haystack.document_stores.in_memory import InMemoryDocumentStore

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆã‚¢
document_store = InMemoryDocumentStore()

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
indexing = Pipeline()
indexing.add_component("converter", MarkdownToDocument())
indexing.add_component("splitter", DocumentSplitter(split_by="sentence", split_length=10))
indexing.add_component("embedder", OpenAIDocumentEmbedder())
indexing.add_component("writer", DocumentWriter(document_store=document_store))
indexing.connect("converter", "splitter")
indexing.connect("splitter", "embedder")
indexing.connect("embedder", "writer")

# RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
rag = Pipeline()
rag.add_component("embedder", OpenAITextEmbedder())
rag.add_component("retriever", InMemoryEmbeddingRetriever(document_store=document_store))
rag.add_component("prompt", PromptBuilder(template="..."))
rag.add_component("llm", OpenAIGenerator(model="gpt-4o"))
# ... connect components
```

### åˆ¤æ–­åŸºæº–

```
LangChainã‚’é¸ã¶ã¹ãå ´åˆ:
â”œâ”€â”€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ãŒå¿…è¦ï¼ˆãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã€ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—æ¨è«–ï¼‰
â”œâ”€â”€ è±Šå¯Œãªçµ±åˆãŒå¿…è¦ï¼ˆ100ä»¥ä¸Šã®ã‚µãƒ¼ãƒ“ã‚¹é€£æºï¼‰
â”œâ”€â”€ LangGraphã§è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã—ãŸã„
â””â”€â”€ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’æ´»ç”¨ã—ãŸã„

LlamaIndexã‚’é¸ã¶ã¹ãå ´åˆ:
â”œâ”€â”€ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã®ç²¾åº¦ã‚’æœ€å¤§åŒ–ã—ãŸã„
â”œâ”€â”€ è¤‡é›‘ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹é€ ï¼ˆãƒ„ãƒªãƒ¼ã€ã‚°ãƒ©ãƒ•ï¼‰ãŒå¿…è¦
â”œâ”€â”€ LlamaParse/LlamaCloudã¨ã®çµ±åˆ
â””â”€â”€ ã‚·ãƒ³ãƒ—ãƒ«ãªAPIã§ç´ æ—©ãå®Ÿè£…ã—ãŸã„

Haystackã‚’é¸ã¶ã¹ãå ´åˆ:
â”œâ”€â”€ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºè¦ä»¶ï¼ˆç›£æŸ»ã€ã‚¬ãƒãƒŠãƒ³ã‚¹ï¼‰
â”œâ”€â”€ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å¯è¦–åŒ–ãƒ»ãƒ‡ãƒãƒƒã‚°ãŒé‡è¦
â”œâ”€â”€ ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹/è¦åˆ¶ç”£æ¥­ã§ã®é‹ç”¨
â””â”€â”€ deepset Cloudã¨ã®çµ±åˆ
```

---

## ãƒ™ã‚¯ãƒˆãƒ«DBé¸å®šï¼šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚³ã‚¹ãƒˆ

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

100ä¸‡ãƒ™ã‚¯ãƒˆãƒ«ã€768æ¬¡å…ƒã€top-k=10ã§ã®æ¤œç´¢æ€§èƒ½ï¼š

| DB | QPS | P99 Latency | ãƒ¡ãƒ¢ãƒª | æœˆé¡ã‚³ã‚¹ãƒˆ* |
|----|-----|-------------|--------|------------|
| **Qdrant** | 8,500 | 12ms | 4.2GB | $0 (self-host) |
| **Milvus** | 7,200 | 15ms | 5.1GB | $0 (self-host) |
| **Pinecone** | 6,800 | 18ms | - | $70 |
| **Weaviate** | 5,900 | 22ms | 4.8GB | $0 (self-host) |
| **Chroma** | 3,200 | 45ms | 6.2GB | $0 |
| **pgvector** | 2,100 | 68ms | 3.8GB | $0 |

*Pineconeã¯s1.x1 podã€100ä¸‡ãƒ™ã‚¯ãƒˆãƒ«æƒ³å®š

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ¥æ¨å¥¨

**ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— / MVP**
```yaml
æ¨å¥¨: Chroma or pgvector
ç†ç”±:
  - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒç°¡å˜
  - è¿½åŠ ã‚¤ãƒ³ãƒ•ãƒ©ä¸è¦
  - 10ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¾ã§ã¯ååˆ†ãªæ€§èƒ½

å®Ÿè£…ä¾‹:
  # pgvector (æ—¢å­˜PostgreSQLã«è¿½åŠ )
  CREATE EXTENSION vector;
  CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(1536)
  );
  CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);
```

**æœ¬ç•ªç’°å¢ƒ / ã‚¹ã‚±ãƒ¼ãƒ«é‡è¦–**
```yaml
æ¨å¥¨: Qdrant or Milvus
ç†ç”±:
  - æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ
  - é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
  - æœ¬ç•ªå®Ÿç¸¾è±Šå¯Œ

Qdrantã®å ´åˆ:
  from qdrant_client import QdrantClient
  from qdrant_client.models import Distance, VectorParams, PointStruct

  client = QdrantClient(host="localhost", port=6333)

  # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆé‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒª50%å‰Šæ¸›ï¼‰
  client.create_collection(
      collection_name="documents",
      vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
      quantization_config=models.ScalarQuantization(
          scalar=models.ScalarQuantizationConfig(
              type=models.ScalarType.INT8,
              always_ram=True
          )
      )
  )
```

**ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º / ãƒãƒãƒ¼ã‚¸ãƒ‰å¸Œæœ›**
```yaml
æ¨å¥¨: Pinecone
ç†ç”±:
  - é‹ç”¨è² è·ã‚¼ãƒ­
  - SLAã‚ã‚Š
  - SOC2/GDPRå¯¾å¿œ

æ³¨æ„ç‚¹:
  - ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆ100ä¸‡ãƒ™ã‚¯ãƒˆãƒ«ã§æœˆ$70ã€œï¼‰
  - ãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³
  - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯è‡ªå‰ã‚ˆã‚Šé…ã„
```

---

## æ¤œç´¢ç²¾åº¦ã‚’ä¸Šã’ã‚‹: Hybrid Search + Reranking

Naive RAGã®æœ€å¤§ã®å•é¡Œã¯**æ¤œç´¢ç²¾åº¦**ã§ã™ã€‚

### ç²¾åº¦æ¯”è¼ƒï¼ˆBEIR benchmarkã‚ˆã‚Šï¼‰

| æ‰‹æ³• | NDCG@10 | å®Ÿè£…é›£æ˜“åº¦ |
|------|---------|-----------|
| BM25ã®ã¿ | 0.42 | ä½ |
| Embeddingæ¤œç´¢ã®ã¿ | 0.48 | ä½ |
| **Hybrid (BM25 + Embedding)** | 0.54 | ä¸­ |
| **Hybrid + Reranking** | 0.61 | ä¸­ |
| Hybrid + ColBERT Reranking | 0.64 | é«˜ |

### å®Ÿè£…ä¾‹ï¼šQdrant + Hybrid Search + Cohere Rerank

```python
from qdrant_client import QdrantClient
import cohere

# 1. Hybrid Searchï¼ˆBM25 + ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼‰
results = qdrant_client.query_points(
    collection_name="documents",
    query=query_embedding,
    query_filter=None,
    limit=20,  # Rerankingã®ãŸã‚å¤šã‚ã«å–å¾—
    with_payload=True,
    search_params=models.SearchParams(
        quantization=models.QuantizationSearchParams(
            ignore=False,
            rescore=True,  # é‡å­åŒ–å¾Œã«å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        )
    )
).points

# 2. Cohere Reranking
co = cohere.Client(api_key="...")
reranked = co.rerank(
    model="rerank-v3.5",
    query=query,
    documents=[r.payload["content"] for r in results],
    top_n=5
)

# 3. Rerankã•ã‚ŒãŸçµæœã‚’ä½¿ç”¨
final_docs = [results[r.index] for r in reranked.results]
```

### Rerankingãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | ç²¾åº¦ | é€Ÿåº¦ | ã‚³ã‚¹ãƒˆ |
|--------|------|------|--------|
| Cohere rerank-v3.5 | æœ€é«˜ | é€Ÿã„ | $1/1000ã‚¯ã‚¨ãƒª |
| Jina Reranker v2 | é«˜ | é€Ÿã„ | ç„¡æ–™æ ã‚ã‚Š |
| BGE Reranker | é«˜ | ä¸­ | ç„¡æ–™ (self-host) |
| ColBERT (RAGatouille) | æœ€é«˜ | é…ã„ | ç„¡æ–™ (self-host) |

---

## PDFå‡¦ç†ã®é—˜ã„ï¼šDocling vs Unstructured vs LlamaParse

**PDFã¯åœ°ç„**ã§ã™ã€‚ç‰¹ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã¨è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€‚

### ç²¾åº¦æ¯”è¼ƒï¼ˆ100ãƒšãƒ¼ã‚¸ã®è²¡å‹™å ±å‘Šæ›¸ã§æ¤œè¨¼ï¼‰

| ãƒ„ãƒ¼ãƒ« | ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º | ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆä¿æŒ | å‡¦ç†é€Ÿåº¦ | ã‚³ã‚¹ãƒˆ |
|--------|------------|--------------|---------|--------|
| **Docling** | 95% | 90% | 2.3 pages/s | ç„¡æ–™ |
| LlamaParse | 92% | 95% | 1.8 pages/s | $0.003/page |
| Unstructured | 85% | 80% | 3.1 pages/s | ç„¡æ–™ |
| PyPDF | 60% | 40% | 8.2 pages/s | ç„¡æ–™ |

### Doclingã®å®Ÿè£…

```python
from docling.document_converter import DocumentConverter
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.base_models import InputFormat

# ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã‚’æœ‰åŠ¹åŒ–
pipeline_options = PdfPipelineOptions()
pipeline_options.do_table_structure = True
pipeline_options.table_structure_options.do_cell_matching = True

converter = DocumentConverter(
    allowed_formats=[InputFormat.PDF],
    format_options={InputFormat.PDF: pipeline_options}
)

# å¤‰æ›
result = converter.convert("financial_report.pdf")

# Markdownå‡ºåŠ›
markdown = result.document.export_to_markdown()

# ãƒ†ãƒ¼ãƒ–ãƒ«ã¯Markdownå½¢å¼ã§ä¿æŒã•ã‚Œã‚‹
# | é …ç›® | 2024å¹´ | 2025å¹´ |
# |------|--------|--------|
# | å£²ä¸Š | 100å„„  | 120å„„  |
```

---

## è©•ä¾¡ãªã—ã®RAGã¯å±é™ºï¼šRAGASå®Ÿè£…ã‚¬ã‚¤ãƒ‰

**æœ¬ç•ªç’°å¢ƒã§è©•ä¾¡ãªã—ã«RAGã‚’å‹•ã‹ã™ã®ã¯ã€ãƒ†ã‚¹ãƒˆãªã—ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã®ã¨åŒã˜**ã§ã™ã€‚

### RAGASã®4ã¤ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,       # ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã‹
    answer_relevancy,   # å›ç­”ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹
    context_precision,  # å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç²¾åº¦
    context_recall      # å¿…è¦ãªæƒ…å ±ã‚’å–å¾—ã§ããŸã‹
)
from datasets import Dataset

# è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
eval_data = {
    "question": ["RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", ...],
    "answer": ["RAGã¯...", ...],
    "contexts": [["æ–‡è„ˆ1", "æ–‡è„ˆ2"], ...],
    "ground_truth": ["æ­£è§£1", ...]
}
dataset = Dataset.from_dict(eval_data)

# è©•ä¾¡å®Ÿè¡Œ
results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
)

print(results)
# {'faithfulness': 0.87, 'answer_relevancy': 0.91,
#  'context_precision': 0.82, 'context_recall': 0.78}
```

### ç›®æ¨™å€¤ã®ç›®å®‰

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æœ€ä½ãƒ©ã‚¤ãƒ³ | ç›®æ¨™å€¤ | å„ªç§€ |
|-----------|----------|--------|------|
| Faithfulness | 0.7 | 0.85 | 0.95+ |
| Answer Relevancy | 0.7 | 0.85 | 0.95+ |
| Context Precision | 0.6 | 0.75 | 0.90+ |
| Context Recall | 0.6 | 0.75 | 0.90+ |

### CI/CDã¸ã®çµ„ã¿è¾¼ã¿

```yaml
# .github/workflows/rag-eval.yml
name: RAG Evaluation
on:
  push:
    paths:
      - 'src/rag/**'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run RAGAS evaluation
        run: |
          python scripts/evaluate_rag.py

      - name: Check thresholds
        run: |
          python -c "
          import json
          with open('eval_results.json') as f:
              results = json.load(f)
          assert results['faithfulness'] >= 0.8, 'Faithfulness below threshold'
          assert results['answer_relevancy'] >= 0.8, 'Relevancy below threshold'
          "
```

---

## æœ¬ç•ªã§é­é‡ã™ã‚‹è½ã¨ã—ç©´ã¨å¯¾ç­–

### 1. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®ç½ 

```
å•é¡Œ: ãƒãƒ£ãƒ³ã‚¯ãŒå¤§ãã™ãã‚‹ â†’ ãƒã‚¤ã‚ºæ··å…¥
     ãƒãƒ£ãƒ³ã‚¯ãŒå°ã•ã™ãã‚‹ â†’ æ–‡è„ˆå–ªå¤±

å¯¾ç­–: Semantic Chunkingã‚’ä½¿ã†
```

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# æ„å‘³çš„ãªåŒºåˆ‡ã‚Šã§ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
splitter = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",
    breakpoint_threshold_amount=95
)
chunks = splitter.split_documents(documents)
```

### 2. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

```
å•é¡Œ: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ã—ãŸã‚‰å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå£Šã‚Œã‚‹

å¯¾ç­–: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä»˜ãã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
```

```python
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã«ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å«ã‚ã‚‹
collection_name = f"documents_v{EMBEDDING_MODEL_VERSION}"

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã¯æ–°æ—§ä¸¦è¡Œé‹ç”¨
old_collection = "documents_v1"
new_collection = "documents_v2"
```

### 3. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®å•é¡Œ

```
å•é¡Œ: æ¤œç´¢ + LLMç”Ÿæˆã§3ç§’ä»¥ä¸Šã‹ã‹ã‚‹

å¯¾ç­–: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° + ä¸¦åˆ—æ¤œç´¢
```

```python
import asyncio
from langchain_openai import ChatOpenAI

# æ¤œç´¢ã¨LLMæº–å‚™ã‚’ä¸¦åˆ—åŒ–
async def rag_query(query: str):
    # æ¤œç´¢ã‚’é–‹å§‹
    search_task = asyncio.create_task(
        vectorstore.asimilarity_search(query, k=5)
    )

    # LLMã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
    llm = ChatOpenAI(model="gpt-4o", streaming=True)

    docs = await search_task

    async for chunk in llm.astream(build_prompt(query, docs)):
        yield chunk.content
```

### 4. ã‚³ã‚¹ãƒˆçˆ†ç™º

```
å•é¡Œ: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° + LLMã‚³ã‚¹ãƒˆãŒäºˆæƒ³å¤–ã«é«˜ã„

å¯¾ç­–: ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° + é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠ
```

```python
from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache

# LLMå¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã¯å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§ååˆ†ãªå ´åˆãŒå¤šã„
# text-embedding-3-large ($0.13/1M) vs text-embedding-3-small ($0.02/1M)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
```

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³é›†

### ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚·ãƒ³ãƒ—ãƒ«RAGï¼ˆMVPå‘ã‘ï¼‰

```
User â†’ API â†’ Chroma â†’ GPT-4o â†’ Response

ã‚³ã‚¹ãƒˆ: ã€œ$50/æœˆ
é©ç”¨: POCã€ç¤¾å†…ãƒ„ãƒ¼ãƒ«ã€ã€œ1ä¸‡ã‚¯ã‚¨ãƒª/æœˆ
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³RAG

```
User â†’ API Gateway â†’ Redis Cache
                  â†“ (cache miss)
              Qdrant (Hybrid Search)
                  â†“
              Cohere Rerank
                  â†“
              GPT-4o (streaming)
                  â†“
              RAGAS Eval (async)

ã‚³ã‚¹ãƒˆ: ã€œ$500/æœˆ
é©ç”¨: B2B SaaSã€10ä¸‡ã‚¯ã‚¨ãƒª/æœˆ
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºRAG

```
                    â”Œâ”€ Langfuse (observability)
                    â”‚
User â†’ WAF â†’ Kong â†’ Haystack Pipeline
                    â”‚
                    â”œâ”€ Docling (PDFå‡¦ç†)
                    â”œâ”€ Milvus Cluster (æ¤œç´¢)
                    â”œâ”€ BGE Reranker (self-hosted)
                    â”œâ”€ Azure OpenAI (LLM)
                    â””â”€ RAGAS + Custom Eval

ã‚³ã‚¹ãƒˆ: ã€œ$5,000/æœˆ
é©ç”¨: é‡‘èã€åŒ»ç™‚ã€100ä¸‡ã‚¯ã‚¨ãƒª/æœˆ
```

---

## ãƒ„ãƒ¼ãƒ«é¸å®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```
START
  â”‚
  â”œâ”€ äºˆç®—ã¯ï¼Ÿ
  â”‚   â”œâ”€ ãªã— â†’ Chroma + LlamaIndex + GPT-4o-mini
  â”‚   â”œâ”€ ã€œ$100/æœˆ â†’ pgvector + LangChain + GPT-4o
  â”‚   â””â”€ $1000+/æœˆ â†’ æ¬¡ã®è³ªå•ã¸
  â”‚
  â”œâ”€ é‹ç”¨è² è·ã¯è¨±å®¹ã§ãã‚‹ï¼Ÿ
  â”‚   â”œâ”€ NO â†’ Pinecone + LangChain + OpenAI
  â”‚   â””â”€ YES â†’ æ¬¡ã®è³ªå•ã¸
  â”‚
  â”œâ”€ PDFå‡¦ç†ãŒå¤šã„ï¼Ÿ
  â”‚   â”œâ”€ YES â†’ Docling + RAGFlow
  â”‚   â””â”€ NO â†’ æ¬¡ã®è³ªå•ã¸
  â”‚
  â”œâ”€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ãŒå¿…è¦ï¼Ÿ
  â”‚   â”œâ”€ YES â†’ LangGraph + LangChain
  â”‚   â””â”€ NO â†’ LlamaIndex
  â”‚
  â””â”€ è¦åˆ¶ç”£æ¥­ï¼Ÿ
      â”œâ”€ YES â†’ Haystack + ã‚ªãƒ³ãƒ—ãƒ¬
      â””â”€ NO â†’ å¥½ã¿ã§é¸æŠ
```

---

## å…¨ãƒ„ãƒ¼ãƒ«ã‚«ã‚¿ãƒ­ã‚°

45ä»¥ä¸Šã®ãƒ„ãƒ¼ãƒ«ã‚’7ã‚«ãƒ†ã‚´ãƒªã§æ•´ç†ã—ã¾ã—ãŸï¼š

https://rag-catalog.vercel.app

- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: 15ãƒ„ãƒ¼ãƒ«
- **ãƒ™ã‚¯ãƒˆãƒ«DB**: 6ãƒ„ãƒ¼ãƒ«
- **è©•ä¾¡**: 6ãƒ„ãƒ¼ãƒ«
- **ãƒ‡ãƒ¼ã‚¿æº–å‚™**: 5ãƒ„ãƒ¼ãƒ«
- **ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°**: 6ãƒ„ãƒ¼ãƒ«
- **ã‚¨ãƒ³ã‚¸ãƒ³**: 3ãƒ„ãƒ¼ãƒ«
- **ãƒªã‚½ãƒ¼ã‚¹**: 4ãƒ„ãƒ¼ãƒ«

ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ã‚½ãƒ¼ãƒˆã€è©³ç´°æƒ…å ±ã™ã¹ã¦æƒã£ã¦ã„ã¾ã™ã€‚

---

## ã¾ã¨ã‚

RAGãƒ„ãƒ¼ãƒ«é¸å®šã®éµï¼š

1. **è¦ä»¶ã‚’æ˜ç¢ºã«** - MVPï¼Ÿæœ¬ç•ªï¼Ÿã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºï¼Ÿ
2. **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å–ã‚‹** - è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼
3. **è©•ä¾¡ã‚’çµ„ã¿è¾¼ã‚€** - RAGASã¯å¿…é ˆ
4. **æ®µéšçš„ã«é«˜åº¦åŒ–** - Naive RAG â†’ Hybrid â†’ Reranking

è¿·ã£ãŸã‚‰ã‚«ã‚¿ãƒ­ã‚°ã§æ¢ã—ã¦ãã ã•ã„ï¼š
https://rag-catalog.vercel.app

:::message
ãƒ„ãƒ¼ãƒ«è¿½åŠ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯GitHub Issueã§å—ã‘ä»˜ã‘ã¦ã„ã¾ã™ã€‚
https://github.com/babushkai/ragcatalog
:::
