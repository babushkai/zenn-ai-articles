---
title: "Advanced RAGå®Œå…¨ã‚¬ã‚¤ãƒ‰ - RAPTOR/Hybrid Search/Rerankingã®å®Ÿè£…"
emoji: "ğŸ”¬"
type: "tech"
topics: ["rag", "llm", "vectordb", "æ¤œç´¢", "ai"]
published: false
---

**ã€ŒRAGã‚’å®Ÿè£…ã—ãŸã‘ã©ã€å›ç­”ã®ç²¾åº¦ãŒã‚¤ãƒã‚¤ãƒã€**

ãã‚Œã€**Naive RAG**ã®é™ç•Œã§ã™ã€‚

2026å¹´ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«ã¯ã€RAPTORã€Hybrid Searchã€Rerankingãªã©ã®é«˜åº¦ãªæŠ€è¡“ãŒå¿…é ˆã«ãªã£ã¦ã„ã¾ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€RAGã®ç²¾åº¦ã‚’åŠ‡çš„ã«å‘ä¸Šã•ã›ã‚‹æŠ€è¡“ã‚’ä½“ç³»çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## RAGã®é€²åŒ–

```
Naive RAG (2023)
â”œâ”€â”€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² â†’ åŸ‹ã‚è¾¼ã¿ â†’ Top-Kæ¤œç´¢ â†’ LLMç”Ÿæˆ
â”‚   å•é¡Œ: ç²¾åº¦ãŒä½ã„ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¬ è½

Advanced RAG (2024-2025)
â”œâ”€â”€ é«˜åº¦ãªãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
â”œâ”€â”€ Hybrid Search (Keyword + Semantic)
â”œâ”€â”€ Reranking
â”‚   æ”¹å–„: ç²¾åº¦å‘ä¸Šã€ãŸã ã—å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…

Modular RAG / RAPTOR (2025-2026)
â”œâ”€â”€ éšå±¤çš„è¦ç´„
â”œâ”€â”€ çŸ¥è­˜ã‚°ãƒ©ãƒ•çµ±åˆ
â”œâ”€â”€ å‹•çš„æ¤œç´¢æˆ¦ç•¥
    æ”¹å–„: ãƒãƒ«ãƒãƒ›ãƒƒãƒ—æ¨è«–ã€æ–‡æ›¸é–“ã®é–¢ä¿‚ç†è§£
```

---

## ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥

### ãªãœãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ãŒé‡è¦ã‹

:::message
**ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã®æœ¬è³ª:** æƒ…å ±ã®ã€Œç²’åº¦ã€ã‚’æ±ºå®šã™ã‚‹ã€‚ç²—ã™ãã‚‹ã¨ãƒã‚¤ã‚ºã€ç´°ã‹ã™ãã‚‹ã¨æ–‡è„ˆå–ªå¤±ã€‚
:::

### æˆ¦ç•¥æ¯”è¼ƒ

| æˆ¦ç•¥ | ç‰¹å¾´ | æœ€é©ãªã‚±ãƒ¼ã‚¹ |
|------|------|-------------|
| Fixed-size | ã‚·ãƒ³ãƒ—ãƒ«ã€ä¸€è²«æ€§ | å‡è³ªãªãƒ†ã‚­ã‚¹ãƒˆ |
| Sentence-aware | æ–‡å¢ƒç•Œã‚’å°Šé‡ | è‡ªç„¶æ–‡æ›¸ |
| Semantic | æ„å‘³çš„ã¾ã¨ã¾ã‚Š | è¤‡é›‘ãªæ–‡æ›¸ |
| Document-aware | æ§‹é€ ã‚’ç¶­æŒ | æŠ€è¡“æ–‡æ›¸ã€è«–æ–‡ |
| Recursive | éšå±¤çš„åˆ†å‰² | é•·æ–‡æ›¸ |

### å®Ÿè£…

#### 1. Fixed-size Chunking

```python
def fixed_size_chunking(text, chunk_size=512, overlap=50):
    """å›ºå®šã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

    return chunks
```

**å•é¡Œç‚¹:** æ–‡ã®é€”ä¸­ã§åˆ‡ã‚Œã‚‹ã€æ„å‘³çš„ãªã¾ã¨ã¾ã‚ŠãŒç„¡è¦–ã•ã‚Œã‚‹

#### 2. Semantic Chunking

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class SemanticChunker:
    def __init__(self, model_name="all-MiniLM-L6-v2", threshold=0.5):
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold

    def chunk(self, text):
        # 1. æ–‡ã«åˆ†å‰²
        sentences = self._split_sentences(text)

        # 2. å„æ–‡ã‚’åŸ‹ã‚è¾¼ã¿
        embeddings = self.model.encode(sentences)

        # 3. éš£æ¥æ–‡ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = self._cosine_similarity(embeddings[i], embeddings[i + 1])
            similarities.append(sim)

        # 4. é¡ä¼¼åº¦ãŒé–¾å€¤ã‚’ä¸‹å›ã‚‹ç®‡æ‰€ã§åˆ†å‰²
        chunks = []
        current_chunk = [sentences[0]]

        for i, sim in enumerate(similarities):
            if sim < self.threshold:
                # æ„å‘³çš„ãªåˆ‡ã‚Œç›®
                chunks.append(" ".join(current_chunk))
                current_chunk = [sentences[i + 1]]
            else:
                current_chunk.append(sentences[i + 1])

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks
```

#### 3. Document-aware Chunking

æ§‹é€ ï¼ˆè¦‹å‡ºã—ã€æ®µè½ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼‰ã‚’å°Šé‡ï¼š

```python
class DocumentAwareChunker:
    def chunk(self, document):
        chunks = []

        # Markdownã®å ´åˆ
        if document.type == "markdown":
            sections = self._split_by_headers(document.content)
            for section in sections:
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã‚’ã•ã‚‰ã«åˆ†å‰²ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if len(section.content) > self.max_chunk_size:
                    sub_chunks = self._split_paragraphs(section.content)
                    for sub in sub_chunks:
                        chunks.append(Chunk(
                            content=sub,
                            metadata={
                                "header": section.header,
                                "level": section.level,
                            }
                        ))
                else:
                    chunks.append(Chunk(
                        content=section.content,
                        metadata={"header": section.header}
                    ))

        return chunks
```

#### 4. Contextual Chunkingï¼ˆAnthropicæ–¹å¼ï¼‰

å„ãƒãƒ£ãƒ³ã‚¯ã«**æ–‡è„ˆæƒ…å ±ã‚’ä»˜åŠ **ï¼š

```python
class ContextualChunker:
    def __init__(self, llm):
        self.llm = llm

    async def chunk_with_context(self, document, chunks):
        """å„ãƒãƒ£ãƒ³ã‚¯ã«æ–‡æ›¸å…¨ä½“ã®æ–‡è„ˆã‚’ä»˜åŠ """
        contextualized = []

        for chunk in chunks:
            # LLMã§æ–‡è„ˆã‚’ç”Ÿæˆ
            context = await self.llm.generate(
                prompt=f"""
                <document>
                {document[:2000]}...  # æ–‡æ›¸ã®å†’é ­
                </document>

                <chunk>
                {chunk.content}
                </chunk>

                ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡æ›¸å…¨ä½“ã®æ–‡è„ˆã§èª¬æ˜ã™ã‚‹çŸ­ã„æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
                ã€Œã“ã®ãƒãƒ£ãƒ³ã‚¯ã¯...ã«ã¤ã„ã¦è¿°ã¹ã¦ã„ã¾ã™ã€ã¨ã„ã†å½¢å¼ã§ã€‚
                """
            )

            contextualized.append(Chunk(
                content=f"{context}\n\n{chunk.content}",
                metadata=chunk.metadata,
            ))

        return contextualized
```

**åŠ¹æœ:** æ¤œç´¢ç²¾åº¦ãŒå¤§å¹…ã«å‘ä¸Šï¼ˆç‰¹ã«vague queryã«å¯¾ã—ã¦ï¼‰

---

## Hybrid Search

### å•é¡Œ: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®é™ç•Œ

```
ã‚¯ã‚¨ãƒª: "ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ E1234 ã®è§£æ±ºæ–¹æ³•"

ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®çµæœ:
1. "ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼è§£æ±ºã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ" (é¡ä¼¼åº¦: 0.82)
2. "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰" (é¡ä¼¼åº¦: 0.78)
3. "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹" (é¡ä¼¼åº¦: 0.75)

â†’ "E1234" ã¨ã„ã†å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒç„¡è¦–ã•ã‚Œã¦ã„ã‚‹ï¼
```

### è§£æ±º: Keyword + Semantic

```python
class HybridSearch:
    def __init__(self, vector_store, bm25_index):
        self.vector_store = vector_store
        self.bm25 = bm25_index

    def search(self, query, k=10, alpha=0.5):
        """
        alpha: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®é‡ã¿ (0-1)
        1-alpha: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®é‡ã¿
        """
        # 1. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        semantic_results = self.vector_store.search(
            query_embedding=self._embed(query),
            k=k * 2,  # å¤šã‚ã«å–å¾—
        )

        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ (BM25)
        keyword_results = self.bm25.search(
            query=query,
            k=k * 2,
        )

        # 3. Reciprocal Rank Fusion (RRF) ã§ãƒãƒ¼ã‚¸
        fused = self._rrf_fusion(
            semantic_results,
            keyword_results,
            alpha=alpha,
        )

        return fused[:k]

    def _rrf_fusion(self, results1, results2, alpha, k=60):
        """RRFã§ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ"""
        scores = {}

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯çµæœã®ã‚¹ã‚³ã‚¢
        for rank, doc in enumerate(results1):
            rrf_score = alpha * (1 / (k + rank + 1))
            scores[doc.id] = scores.get(doc.id, 0) + rrf_score

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµæœã®ã‚¹ã‚³ã‚¢
        for rank, doc in enumerate(results2):
            rrf_score = (1 - alpha) * (1 / (k + rank + 1))
            scores[doc.id] = scores.get(doc.id, 0) + rrf_score

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [self._get_doc(doc_id) for doc_id, _ in sorted_docs]
```

### alphaã®èª¿æ•´æŒ‡é‡

| ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ— | alphaå€¤ | ç†ç”± |
|-------------|---------|------|
| æ¦‚å¿µçš„ãªè³ªå• | 0.7-0.8 | æ„å‘³çš„ãªé¡ä¼¼æ€§ãŒé‡è¦ |
| å…·ä½“çš„ãªæ¤œç´¢ | 0.3-0.4 | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãŒé‡è¦ |
| ã‚³ãƒ¼ãƒ‰æ¤œç´¢ | 0.2-0.3 | é–¢æ•°åã€å¤‰æ•°åã®ä¸€è‡´ãŒé‡è¦ |
| ãƒãƒ©ãƒ³ã‚¹å‹ | 0.5 | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |

---

## Reranking

### å•é¡Œ: åˆæœŸæ¤œç´¢ã®é™ç•Œ

åˆæœŸæ¤œç´¢ï¼ˆbi-encoderï¼‰ã¯**é«˜é€Ÿã ãŒç²—ã„**ï¼š

```
æ¤œç´¢ã‚¯ã‚¨ãƒª: "Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’éåŒæœŸã§èª­ã¿è¾¼ã‚€æ–¹æ³•"

Top-10çµæœï¼ˆæ¤œç´¢å¾Œã€LLMã«æ¸¡ã™å‰ï¼‰:
1. "Python asyncioå…¥é–€" - é–¢é€£åº¦: ä¸­
2. "ãƒ•ã‚¡ã‚¤ãƒ«I/Oã®åŸºæœ¬" - é–¢é€£åº¦: ä½
3. "éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°æ¦‚è«–" - é–¢é€£åº¦: ä¸­
4. "aiofilesãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä½¿ã„æ–¹" - é–¢é€£åº¦: é«˜ â˜…
5. ...

â†’ 4ç•ªç›®ãŒæœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„ã®ã«ã€1ç•ªç›®ã§ã¯ãªã„
```

### è§£æ±º: Cross-Encoder Reranking

```python
from sentence_transformers import CrossEncoder

class Reranker:
    def __init__(self, model_name="cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model = CrossEncoder(model_name)

    def rerank(self, query, documents, top_k=5):
        """
        Cross-Encoderã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
        """
        # ã‚¯ã‚¨ãƒªã¨å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒšã‚¢ã‚’ä½œæˆ
        pairs = [(query, doc.content) for doc in documents]

        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆCross-Encoderã¯ç²¾åº¦ãŒé«˜ã„ãŒé…ã„ï¼‰
        scores = self.model.predict(pairs)

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)

        return [doc for doc, _ in doc_scores[:top_k]]
```

### ColBERT: Late Interaction

Cross-Encoderã¯ç²¾åº¦ãŒé«˜ã„ãŒ**é…ã„**ã€‚ColBERTã¯å¦¥å”ç‚¹ï¼š

```python
class ColBERTReranker:
    """
    Late Interaction: ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼åº¦è¨ˆç®—
    Cross-Encoderã‚ˆã‚Šé«˜é€Ÿã€Bi-Encoderã‚ˆã‚Šé«˜ç²¾åº¦
    """

    def __init__(self, model):
        self.model = model

    def rerank(self, query, documents):
        # 1. ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§åŸ‹ã‚è¾¼ã¿
        query_embeddings = self.model.encode_query(query)  # shape: (q_len, dim)

        scores = []
        for doc in documents:
            # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§åŸ‹ã‚è¾¼ã¿
            doc_embeddings = self.model.encode_doc(doc.content)  # shape: (d_len, dim)

            # 3. MaxSim: å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã«æœ€ã‚‚è¿‘ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹
            # score = sum of max similarities for each query token
            similarity_matrix = query_embeddings @ doc_embeddings.T
            max_sims = similarity_matrix.max(dim=1).values
            score = max_sims.sum()

            scores.append((doc, score))

        # ã‚½ãƒ¼ãƒˆ
        scores.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in scores]
```

### Rerankingãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
class RAGPipeline:
    def __init__(self, retriever, reranker, llm):
        self.retriever = retriever
        self.reranker = reranker
        self.llm = llm

    async def query(self, question):
        # 1. åˆæœŸæ¤œç´¢ï¼ˆåºƒãã€å¤šã‚ã«ï¼‰
        initial_results = await self.retriever.search(
            question,
            k=50,  # å¤šã‚ã«å–å¾—
        )

        # 2. ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
        reranked = self.reranker.rerank(
            question,
            initial_results,
            top_k=5,  # çµã‚Šè¾¼ã¿
        )

        # 3. LLMç”Ÿæˆ
        context = self._format_context(reranked)
        response = await self.llm.generate(
            system="ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚",
            user=f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}\n\nè³ªå•: {question}",
        )

        return response
```

---

## RAPTOR: éšå±¤çš„æ¤œç´¢

### å•é¡Œ: ãƒ•ãƒ©ãƒƒãƒˆãªãƒãƒ£ãƒ³ã‚¯ã®é™ç•Œ

```
è³ªå•: "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’èª¬æ˜ã—ã¦"

å¾“æ¥ã®RAG:
- å€‹åˆ¥ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
- å…¨ä½“åƒãŒè¦‹ãˆãªã„
- é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãŒæ•£åœ¨
```

### RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)

**ã‚¢ã‚¤ãƒ‡ã‚¢:** ãƒãƒ£ãƒ³ã‚¯ã‚’éšå±¤çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»è¦ç´„ã—ã€æŠ½è±¡åº¦ã®ç•°ãªã‚‹ãƒ¬ãƒ™ãƒ«ã§æ¤œç´¢

```
                    [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®è¦ç´„]
                           Level 3
                    /                    \
        [ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®è¦ç´„]        [ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®è¦ç´„]
              Level 2                    Level 2
           /        \                  /        \
    [Reacté–¢é€£]  [çŠ¶æ…‹ç®¡ç†]      [APIè¨­è¨ˆ]   [DBè¨­è¨ˆ]
      Level 1     Level 1        Level 1    Level 1
       /   \         |             |          |
    [ãƒãƒ£ãƒ³ã‚¯ç¾¤]  [ãƒãƒ£ãƒ³ã‚¯ç¾¤]  [ãƒãƒ£ãƒ³ã‚¯ç¾¤] [ãƒãƒ£ãƒ³ã‚¯ç¾¤]
      Level 0     Level 0      Level 0    Level 0
```

### å®Ÿè£…

```python
class RAPTOR:
    def __init__(self, embedder, llm, vector_store):
        self.embedder = embedder
        self.llm = llm
        self.store = vector_store

    def build_tree(self, documents, max_levels=3):
        """RAPTORãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        # Level 0: å…ƒã®ãƒãƒ£ãƒ³ã‚¯
        current_level = self._chunk_documents(documents)
        all_nodes = list(current_level)

        for level in range(1, max_levels + 1):
            if len(current_level) <= 1:
                break

            # 1. åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—
            embeddings = self.embedder.encode([n.content for n in current_level])

            # 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            clusters = self._cluster(embeddings, min_cluster_size=3)

            # 3. å„ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¦ç´„
            next_level = []
            for cluster_indices in clusters:
                cluster_nodes = [current_level[i] for i in cluster_indices]

                # LLMã§è¦ç´„ã‚’ç”Ÿæˆ
                summary = self._summarize(cluster_nodes)

                summary_node = Node(
                    content=summary,
                    level=level,
                    children=[n.id for n in cluster_nodes],
                )
                next_level.append(summary_node)
                all_nodes.append(summary_node)

            current_level = next_level

        # å…¨ãƒãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜
        self._store_all(all_nodes)

        return all_nodes

    def search(self, query, k=5):
        """
        å…¨ãƒ¬ãƒ™ãƒ«ã‹ã‚‰æ¤œç´¢
        - å…·ä½“çš„ãªè³ªå• â†’ ä¸‹ä½ãƒ¬ãƒ™ãƒ«ï¼ˆè©³ç´°ãªãƒãƒ£ãƒ³ã‚¯ï¼‰
        - æŠ½è±¡çš„ãªè³ªå• â†’ ä¸Šä½ãƒ¬ãƒ™ãƒ«ï¼ˆè¦ç´„ï¼‰
        """
        results = self.store.search(
            query_embedding=self.embedder.encode(query),
            k=k,
            # å…¨ãƒ¬ãƒ™ãƒ«ã‹ã‚‰æ¤œç´¢
        )

        return results

    def _summarize(self, nodes):
        """ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒãƒ¼ãƒ‰ã‚’è¦ç´„"""
        combined = "\n\n".join([n.content for n in nodes])

        prompt = f"""
        ä»¥ä¸‹ã®é–¢é€£ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€é‡è¦ãªæƒ…å ±ã‚’ä¿æŒã—ãªãŒã‚‰è¦ç´„ã—ã¦ãã ã•ã„ã€‚

        ãƒ†ã‚­ã‚¹ãƒˆ:
        {combined}

        è¦ç´„:
        """
        return self.llm.generate(prompt)
```

### RAPTORã®åŠ¹æœ

| ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ— | å¾“æ¥RAG | RAPTOR |
|-------------|---------|--------|
| è©³ç´°ãªè³ªå• | â—‹ | â—‹ |
| å…¨ä½“åƒã®è³ªå• | â–³ | â— |
| ãƒãƒ«ãƒãƒ›ãƒƒãƒ— | â–³ | â—‹ |

---

## é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³

### Query Transformation

æ¤œç´¢å‰ã«ã‚¯ã‚¨ãƒªã‚’å¤‰æ›ï¼š

```python
class QueryTransformer:
    def __init__(self, llm):
        self.llm = llm

    async def expand_query(self, query):
        """ã‚¯ã‚¨ãƒªæ‹¡å¼µ: è¤‡æ•°ã®è¨€ã„æ›ãˆã‚’ç”Ÿæˆ"""
        prompt = f"""
        ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã€åŒã˜æ„å›³ã‚’æŒã¤3ã¤ã®ç•°ãªã‚‹è¨€ã„æ–¹ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

        å…ƒã®ã‚¯ã‚¨ãƒª: {query}

        å¤‰æ›å¾Œ:
        1.
        2.
        3.
        """
        expanded = await self.llm.generate(prompt)
        return self._parse(expanded)

    async def decompose_query(self, query):
        """è¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚’åˆ†è§£"""
        prompt = f"""
        ä»¥ä¸‹ã®è¤‡é›‘ãªè³ªå•ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„å˜ç´”ãªè³ªå•ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

        è³ªå•: {query}

        åˆ†è§£:
        """
        decomposed = await self.llm.generate(prompt)
        return self._parse(decomposed)

    async def hypothetical_document(self, query):
        """HyDE: ä»®æƒ³çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦æ¤œç´¢"""
        prompt = f"""
        ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã™ã‚‹ç†æƒ³çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        ï¼ˆå®Ÿéš›ã®æƒ…å ±ãŒãªãã¦ã‚‚ã€ã‚ã‚‹ã¹ãå›ç­”ã‚’æƒ³åƒã—ã¦ï¼‰

        è³ªå•: {query}

        ç†æƒ³çš„ãªå›ç­”:
        """
        hypothetical = await self.llm.generate(prompt)
        return hypothetical  # ã“ã‚Œã‚’åŸ‹ã‚è¾¼ã¿ã¨ã—ã¦æ¤œç´¢ã«ä½¿ç”¨
```

### Self-RAG

LLMãŒè‡ªåˆ†ã§æ¤œç´¢ã®å¿…è¦æ€§ã‚’åˆ¤æ–­ï¼š

```python
class SelfRAG:
    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    async def generate(self, query):
        # 1. æ¤œç´¢ãŒå¿…è¦ã‹åˆ¤æ–­
        needs_retrieval = await self._judge_retrieval_need(query)

        if needs_retrieval:
            # 2. æ¤œç´¢å®Ÿè¡Œ
            docs = await self.retriever.search(query)

            # 3. å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é–¢é€£æ€§ã‚’è©•ä¾¡
            relevant_docs = []
            for doc in docs:
                is_relevant = await self._judge_relevance(query, doc)
                if is_relevant:
                    relevant_docs.append(doc)

            # 4. å›ç­”ç”Ÿæˆ
            response = await self._generate_with_context(query, relevant_docs)

            # 5. å›ç­”ã®å“è³ªã‚’è‡ªå·±è©•ä¾¡
            quality = await self._judge_quality(query, response, relevant_docs)

            if quality.needs_more_info:
                # è¿½åŠ æ¤œç´¢
                more_docs = await self._search_for_gaps(query, response, quality.gaps)
                response = await self._refine_response(response, more_docs)

            return response
        else:
            # æ¤œç´¢ãªã—ã§ç›´æ¥å›ç­”
            return await self.llm.generate(query)
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

```python
# HNSW (Hierarchical Navigable Small World) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
index_config = {
    "M": 16,           # å„ãƒãƒ¼ãƒ‰ã®æ¥ç¶šæ•°ï¼ˆå¤§ãã„ã»ã©ç²¾åº¦â†‘ã€ãƒ¡ãƒ¢ãƒªâ†‘ï¼‰
    "ef_construction": 200,  # æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…ï¼ˆå¤§ãã„ã»ã©ç²¾åº¦â†‘ã€æ§‹ç¯‰æ™‚é–“â†‘ï¼‰
    "ef_search": 100,  # æ¤œç´¢æ™‚ã®æ¢ç´¢å¹…ï¼ˆå¤§ãã„ã»ã©ç²¾åº¦â†‘ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·â†‘ï¼‰
}
```

### ãƒãƒƒãƒå‡¦ç†

```python
class BatchedRetriever:
    def __init__(self, retriever, batch_size=32):
        self.retriever = retriever
        self.batch_size = batch_size

    async def search_batch(self, queries):
        """è¤‡æ•°ã‚¯ã‚¨ãƒªã‚’ãƒãƒƒãƒå‡¦ç†"""
        results = []

        for i in range(0, len(queries), self.batch_size):
            batch = queries[i:i + self.batch_size]
            # ä¸¦åˆ—å®Ÿè¡Œ
            batch_results = await asyncio.gather(*[
                self.retriever.search(q) for q in batch
            ])
            results.extend(batch_results)

        return results
```

---

## ã¾ã¨ã‚

### RAGæœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
â–¡ ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã¯é©åˆ‡ã‹ï¼Ÿ
  - æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«åˆã£ãŸæ–¹æ³•ã‚’é¸æŠ
  - Contextual chunkingã‚’æ¤œè¨

â–¡ Hybrid Searchã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
  - Keyword + Semantic
  - alphaã‚’é©åˆ‡ã«èª¿æ•´

â–¡ Rerankingã‚’å°å…¥ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
  - Cross-Encoder or ColBERT
  - åˆæœŸæ¤œç´¢ã¯åºƒãã€ãƒªãƒ©ãƒ³ã‚¯ã§çµã‚‹

â–¡ éšå±¤çš„æ¤œç´¢ã‚’æ¤œè¨ã—ãŸã‹ï¼Ÿ
  - RAPTORã§æŠ½è±¡åº¦ã®ç•°ãªã‚‹ãƒ¬ãƒ™ãƒ«
  - å…¨ä½“åƒã‚’å•ã†è³ªå•ã«å¯¾å¿œ

â–¡ ã‚¯ã‚¨ãƒªå¤‰æ›ã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
  - ã‚¯ã‚¨ãƒªæ‹¡å¼µ
  - HyDE
```

### ç²¾åº¦å‘ä¸Šã®å„ªå…ˆé †ä½

1. **ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æœ€é©åŒ–** - åŸºç›¤
2. **Hybrid Search** - å³åŠ¹æ€§ã‚ã‚Š
3. **Reranking** - ç²¾åº¦ã®æ±ºã‚æ‰‹
4. **RAPTOR** - è¤‡é›‘ãªã‚¯ã‚¨ãƒªå¯¾å¿œ
5. **Query Transformation** - ã•ã‚‰ãªã‚‹æ”¹å–„

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [RAG Techniques (GitHub)](https://github.com/NirDiamant/RAG_Techniques)
- [Optimizing RAG with Hybrid Search & Reranking](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
- [Improving RAG with RAPTOR](https://superlinked.com/vectorhub/articles/improve-rag-with-raptor)
- [Advanced RAG Techniques (Neo4j)](https://neo4j.com/blog/genai/advanced-rag-techniques/)

---

**ã‚ãªãŸã®RAGã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã©ã‚“ãªå·¥å¤«ã‚’ã—ã¦ã„ã¾ã™ã‹ï¼Ÿã‚³ãƒ¡ãƒ³ãƒˆã§æ•™ãˆã¦ãã ã•ã„ï¼**
